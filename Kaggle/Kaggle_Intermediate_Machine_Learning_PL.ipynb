{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning\n",
    "\n",
    "Poniższy notebook sporządzono na podstawie kursu dostępnego pod adresem https://www.kaggle.com/learn/intermediate-machine-learning. \n",
    "\n",
    "Tutorial pokazuje m.in.:\n",
    "\n",
    "- jak pracować z danymi kategorycznymi (categorical values) i danymi brakującymi (missing values)\n",
    "- jak poprawić jakość kodu\n",
    "- jak używać zaawansowanych technik walidacji modelu (walidacja krzyżowa)\n",
    "- jak budować modele z użyciem XGBoost\n",
    "- jak unikać częstych błędów (data leakage)\n",
    "\n",
    "Agenda:\n",
    "1. **Introduction**\n",
    "2. **Missing Values**\n",
    "3. **Categorical Variables**\n",
    "4. **Pipelines**\n",
    "5. **Cross-Validation**\n",
    "6. **XGBoost**\n",
    "7. **Data Leakage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) INTRODUCTION\n",
    "Poniższy tutorial działa w oparciu o zbiór danych dot. cen domów w 'Iowa' oraz algorytm Lasów Losowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# wczytanie danych (zbiór jest domyślnie podzielony na część treningową i testową)\n",
    "X_full = pd.read_csv('./iowa_train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('./iowa_test.csv', index_col='Id')\n",
    "\n",
    "# określenie cech i zmiennej przewidywanej\n",
    "y = X_full['SalePrice']\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "\n",
    "# podział domyślnego zbioru treningowego na zbiór treningowy właściwy (jego użyjemy do trenowania modelu) oraz \n",
    "# zbiór walidacyjny (poniżej: test_size)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>11694</td>\n",
       "      <td>2007</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>6600</td>\n",
       "      <td>1962</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>13360</td>\n",
       "      <td>1921</td>\n",
       "      <td>964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>13265</td>\n",
       "      <td>2002</td>\n",
       "      <td>1689</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>13704</td>\n",
       "      <td>2001</td>\n",
       "      <td>1541</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
       "Id                                                                    \n",
       "619    11694       2007      1828         0         2             3   \n",
       "871     6600       1962       894         0         1             2   \n",
       "93     13360       1921       964         0         1             2   \n",
       "818    13265       2002      1689         0         2             3   \n",
       "303    13704       2001      1541         0         2             3   \n",
       "\n",
       "     TotRmsAbvGrd  \n",
       "Id                 \n",
       "619             9  \n",
       "871             5  \n",
       "93              5  \n",
       "818             7  \n",
       "303             6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podgląd danych, których użyjemy do wytrenowania modelu:\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ocena jakości modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# definiujemy 5 różnych modeli Lasów Losowych\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametry 'RandomForestRegressor':\n",
    "\n",
    "- random_state - zapewnia powtarzalność wyników przy każdym uruchomieniu skryptu\n",
    "- n_estimators - liczba drzew w lesie (domyślnie: 100)\n",
    "- criterion - funkcja do pomiaru jakości podziału (opcje: 'mae', 'mse'; domyślnie: 'mse')\n",
    "- max_depth - maksymalna \"głębokość\" drzewa (domyślnie: None)\n",
    "- min_samples_split - minimalna liczba próbek wymagana do podziału węzła (domyślnie: 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu wybrania najlepszego z powyższych modeli zdefiniujemy poniżej funkcję 'score_model()'. Będzie ona zwracać wartość MAE dla zbioru walidacyjnego (tak jak poprzednio, patrz: Introduction_to_ML). Najlepszą wartością MAE dla zbioru walidacyjnego jest wartość najmniejsza!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 24015\n",
      "Model 2 MAE: 23740\n",
      "Model 3 MAE: 23528\n",
      "Model 4 MAE: 23996\n",
      "Model 5 MAE: 23706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# definicja funkcji obliczającej MAE dla modelu:\n",
    "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
    "    model.fit(X_t, y_t) # trenujemy każdy model\n",
    "    preds = model.predict(X_v) # wartości przewidywań dla zbioru walidacyjnego modelu\n",
    "    return mean_absolute_error(y_v, preds) # wartość MAE dla modelu\n",
    "\n",
    "models_mae = {}\n",
    "\n",
    "# iterujemy po modelach:\n",
    "for i in range(len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    models_mae[models[i]] = mae\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wybór najlepszego modelu z punktu widzenia wartości MAE:\n",
    "min_mae = models_mae[model_1]\n",
    "for k, v in models_mae.items():\n",
    "    if v < min_mae:\n",
    "        min_mae = v\n",
    "        best_model = k # model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predykcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiujemy ostateczny model na podstawie powyższej oceny MAE\n",
    "final_model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenujemy model na pełnym (domyślnym) zestawie danych treningowych\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Generujemy prognozy dla domyślnego zbioru testowego\n",
    "preds_test = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisujemy wynik predykcji do pliku CSV\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('iowa_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) MISSING VALUES\n",
    "\n",
    "Większość bibliotek ML (włącznie z scikit-learn) zwraca błąd przy próbie budowy modeli na danych, w których występują braki (NA). Poniżej poruszone zostaną 3 podejścia stosowane do pracy z brakującymi danymi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podejście 1: Usunięcie kolumn z brakującymi danymi\n",
    "\n",
    "Najprostsze rozwiązanie (niekoniecznie najlepsze), to usunięcie kolumn, w których znajdują się braki danych.\n",
    "\n",
    "<img src=\"Images/img_1.jpg\">\n",
    "\n",
    "Niestety, poprzez usuwanie całych kolumn znacznie zmniejsza się nasz zbiór danych pod kątem wyboru cech dla modelu (mamy mniej kolumn do wyboru przez co model może mieć później słabą jakość)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podejście 2: Imputacja - przypisanie nowych wartości\n",
    "\n",
    "Lepsze rozwiązanie niż usuwanie całych kolumn. Polega na zastąpieniu brakujących wartości np. średnią wartością dla danej kolumn\n",
    "\n",
    "<img src=\"Images/img_2.jpg\">\n",
    "\n",
    "Wstawienie nowej wartości nie zawsze jest pożądane, ale zazwyczaj prowadzi do uzyskania modeli o wyższej jakości niż w przypadku usuwania całych kolumn.\n",
    "\n",
    "W przypadku biblioteki Scikit-Learn możliwe jest zastosowanie metody 'SimpleImputer', która zastąpi braki danych (NaN) w danej kolumnie wartością średnią. Przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Wstawianie nowych wartości:\n",
    "#imputer = SimpleImputer()\n",
    "#imputed_X_train_data = pd.DataFrame(imputer.fit_transform(X_train_data))\n",
    "#imputed_X_valid_data = pd.DataFrame(imputer.transform(X_valid_data))\n",
    "\n",
    "## 'Imputacja' usuwa nazwy kolumn; należy wstawić je z powrotem:\n",
    "#imputed_X_train_data.columns = X_train_data.columns\n",
    "#imputed_X_valid_data.columns = X_valid_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podejście 3: Imputacja + INFO - przypisanie nowych wartości oraz uwzgl. info o tym w nowej kolumnie\n",
    "\n",
    "W tym podejściu zastępujemy braki danych, podobnie jak to miało miejsce w punkcie 2, ale oprócz tego dodajemy nową kolumnę  z informacją o tym czy dane w danym wierszu zostały zastąpione nową wartością, czy nie.\n",
    "\n",
    "<img src=\"Images/img_3.jpg\">\n",
    "\n",
    "W pewnych przypadkach, model posiadając wiedzę nt tego, które wartości zostały zastąpione może uzyskiwać lepsze wyniki. W innych - nie zmieni to kompletnie niczego. Przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tworzymy kopie danych, aby ich nie utracić (w razie czego):\n",
    "#X_train_data_plus = X_train_data.copy()\n",
    "#X_valid_data_plus = X_valid_data.copy()\n",
    "\n",
    "## Tworzymy nowe kolumny z informacją (True lub False) czy dane zostały zmienione, czy nie:\n",
    "#for col in cols_with_missing:\n",
    "#    X_train_data_plus[col + '_was_missing'] = X_train_data_plus[col].isnull()\n",
    "#    X_valid_data_plus[col + '_was_missing'] = X_valid_data_plus[col].isnull()\n",
    "\n",
    "## 'Imputacja' - jak wyżej: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PODSUMOWANIE:\n",
    "\n",
    "Podejścia 2 i 3 są lepsze w sytuacjach, gdy usuwając całe kolumny (podejście 1) usuwamy znaczną część danych, co może prowadzić do tego, że dane, które przekażemy do modelu będą niereprezentatywne i przez to model będzie miał słabą jakość. Z kolei, jeśli jakaś kolumna zawiera ponad 50% braków, to niewarto jej \"na siłę\" trzymać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRZYKŁAD\n",
    "#### Zastosowanie powyższych podejść pracy z brakującymi danymi na zbiorze 'Iowa':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytanie (ponowne) danych\n",
    "X_full = pd.read_csv('./iowa_train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('./iowa_test.csv', index_col='Id')\n",
    "\n",
    "# określenie cech i zmiennej przewidywanej oraz usunięcie wierszy, w których nie określono wartości przewidywanej (do \n",
    "# treningu takie dane są bezużyteczne)\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Dla uproszczenia do dalszej analizy użyjemy tylko cech o wartościach numerycznych\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# podział domyślnego zbioru treningowego na zbiór treningowy właściwy i zbiór walidacyjny\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Wstępna ocena danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 36)\n",
      "LotFrontage    212\n",
      "MasVnrArea       6\n",
      "GarageYrBlt     58\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# - wyświetlamy całkowitą ilość wierszy i kolumn (pomoże nam w oszacowaniu ilości brakujących danych)\n",
    "print(X_train.shape)\n",
    "\n",
    "# - liczba brakujących danych w każdej kolumnie zbioru treningowego:\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0]) # tylko kolumny gdzie ilość braków > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wniosek:\n",
    "\n",
    "Zgodnie z powyższym wynikiem mamy stosunkowo niewielką ilość braków danych (dla kolumny LotFrontage odsetek braków jest największy, ale i tak wynosi on < 20% wierszy), dlatego można przypuszczać, że całkowite usunięcie kolumn z brakami danych nie przyniesie dobrych wyników (wyrzucilibyśmy wiele cennych danych, dlatego prawdopodobnie 'imputancja' będzie lepszym rozwiązaniem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Definicja funkcji, która pozwoli porównać różne podejścia pracy z brakami danych\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Rozpatrujemy podejścia:\n",
    "\n",
    "##### - Podejście 1 - usunięcie braków:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Drop columns with missing values):\n",
      "17837.82570776256\n"
     ]
    }
   ],
   "source": [
    "# kolumny z brakami danych:\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# modyfikacja (redukcja) danych - pozbycie się brakujących kolumn:\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "# wartość MAE dla zredukowanych danych:\n",
    "print(\"MAE (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Podejście 2 - imputacja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Imputation):\n",
      "18062.894611872147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputacja:\n",
    "imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "\n",
    "# 'Imputacja' usuwa nazwy kolumn; należy wstawić je z powrotem:\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "# wartość MAE dla zmodyfikowanych danych:\n",
    "print(\"MAE (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WNIOSEK:\n",
    "\n",
    "Wynik jest zaskakujący, ponieważ zakładaliśmy, że przy tak niewielkiej ilości brakujących danych imputacja zwróci lepszy wynik (mniejszą wartość MAE) niż całkowite usunięcie kolumn. Taki wynik może być spowodowany 'szumem' obecnym w danych. Ponadto, może się okazać, że imputacja, która wstawia średnią wartość nie jest odpowiednia dla tego zbioru danych i możliwe, że lepiej byłoby wstawiać w miejsce braków wartości 0 lub wartości najczęściej spotykanej w danej kolumnie itd. Za przykład niech posłuży kolumna 'GarageYrBlt, która wskazuje rok budowy garażu i w której wartość NaN oznacza po prostu brak garażu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Predykcja (z zastosowaniem imputacji z parametrem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputacja\n",
    "final_imputer = SimpleImputer(strategy='median')\n",
    "final_X_train = pd.DataFrame(final_imputer.fit_transform(X_train))\n",
    "final_X_valid = pd.DataFrame(final_imputer.transform(X_valid))\n",
    "\n",
    "# wstawienie nazw kolumn\n",
    "final_X_train.columns = X_train.columns\n",
    "final_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ('final' approach):\n",
      "17791.59899543379\n"
     ]
    }
   ],
   "source": [
    "# definiowanie i trening modelu\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(final_X_train, y_train)\n",
    "\n",
    "# predykcja na zbiorze walidacyjnym i obliczenie wartości MAE\n",
    "preds_valid = model.predict(final_X_valid)\n",
    "print(\"MAE ('final' approach):\")\n",
    "print(mean_absolute_error(y_valid, preds_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputacja dla danych testowych:\n",
    "final_X_test = pd.DataFrame(final_imputer.transform(X_test))\n",
    "\n",
    "# predykcja na zbiorze testowym:\n",
    "preds_test = model.predict(final_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisujemy wynik predykcji do pliku CSV\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('iowa_predictions_miss_vals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) CATEGORICAL VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmienne kategoryczne posiadają organiczoną liczbę wartości. Mogą to być np. odpowiedzi na pytanie ankietowe (\"Nigdy\", \"Rzadko\", \"Często\", \"Codziennie\") czy marki samochodów (Homda, Toyota, Ford). Ogólnie, są jasno określone i jest ich skończona ilość (jasno wskazują daną kategorię).\n",
    "\n",
    "W Pythonie, jeśli zechcemy użyć zmiennych kategorycznych do budowy modelu ML w ich podstawowej formie, to dostaniemy błąd, dlatego należy poddać je \"preprocessingowi\" i **zamienić je na wartości liczbowe**. Wyróżnia się 3 podejścia pracy ze zmiennymi kategorycznymi\n",
    "\n",
    "#### Podejście 1: Usunięcie zmiennych kategorycznych\n",
    "Najłatwiejszym rozwiązaniem jest usunięcie zmiennych (kolumn) zawierających wartości kategoryczne. Działa tylko, jeśli kolumny nie zawierają informacji istotnych z punktu widzenia modelu.\n",
    "\n",
    "#### Podejście 2: Label Encoding\n",
    "Polega na przypisaniu etykietom wartości liczbowych (int). Jest to swego rodzaju porządkowanie etykiet, np. \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "\n",
    "<img src=\"Images/img_4.jpg\">\n",
    "\n",
    "Powyższe podejście ma sens, ponieważ kategorie (etykiety) mają swój \"ranking\". Nie wszystkie zmienne kategoryczne mają jednak wartości, które da się uporządkować (inaczej, nie wszystkie zmienne kategoryczne działają, jak zmienne porządkowe!). W przypadku Drzew i Lasów można jednak zakładać, że kodowanie (jw) zmiennych porządkowych przyniesie dobry wynik.\n",
    "\n",
    "WAŻNE!\n",
    "Jeśli w kolumnie zbioru walidacyjnego występują dane kategoryczne, których wartości nie występują w tej samej kolumnie zbioru treningowego, to otrzymamy BŁĄD! Najprostszym rozwiązaniem w takiej sytuacji jest po prostu całkowite usunięcie problematycznej kolumny (podejście 1).\n",
    "\n",
    "#### Podejście 3: One-Hot Encoding\n",
    "Polega na utworzeniu nowego zestawu kolumn, który jasno określa czy dana wartość zmiennej kategorycznej występuje w zbiorze danych, czy nie.\n",
    "\n",
    "<img src=\"Images/img_5.jpg\">\n",
    "\n",
    "One-Hot Encoding (w odróżnieniu do 'Podejścia 2') nie porządkuje kategorii, dlatego podejście to sprawdza się, gdy nie jest jasny porządek wartości zmiennej kategorycznej (np. \"Red\" nie jest ani większy ani zmiejszy od \"Yellow\") - zmienne kategorycznie nieposiadające \"rankingu\" są określane mianem **zmiennych nominalnych**.\n",
    "\n",
    "Podejście One-Hot Encoding nie działa dobrze, jeśli zmienna kategoryczna przyjmuje zbyt wiele unikalnych wartości (zakłada się, że nie używa się tego podejścia jeśli unikalnych wartości > 15 - w celu sprawdzenia liczby unikalnych wartości:\n",
    "\n",
    "DataFrame['Kolumna'].unique() )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykład:\n",
    "Zazwyczaj zmienne kategoryczne są reprezentowane przez kolumny zawierające wartości tekstowe (w pandas: 'object'). W celu określenia zmiennych kategorycznych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## zwracamy listę zmiennych kategorycznych przy użyciu metody 'dtypes':\n",
    "#s = (X_train.dtypes == 'object')\n",
    "#object_cols = list(s[s].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Podejście 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "# drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "## Określamy wartość MAE po usunięciu zmiennych kategorycznych:\n",
    "#print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "#print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Podejście 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklear.preprocessing import LabelEncoder\n",
    "\n",
    "## Tworzymy kopię danych (w razie wu):\n",
    "#label_X_train = X_train.copy()\n",
    "#label_X_valid = X_valid.copy()\n",
    "\n",
    "##### Aplikujemy metodę 'LabelEncoder' dla każdej kolumny zawierającej dane kategoryczne (określono wcześniej):\n",
    "#label_encoder = LabelEncoder()\n",
    "#for col in object_cols:\n",
    "#    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "#    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "## Określamy wartość MAE:\n",
    "#print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "#print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Podejście 3:\n",
    "\n",
    "W celu skorzystania z metody 'OneHotEncoder' modułu Scikit-Learn należy określić wartości m.in. poniższych parametrów:\n",
    "\n",
    "- handle_unknown=ignore - pozwala uniknąć błędów, jeśli zbiór walidacyjny zawiera klasy, które nie występowały w zbiorze treningowym;\n",
    "- sparse=False - zapewnia, że kolumny, które poddajemy \"kodowaniu\" będą miały format 'numpy array' zamiast 'sparse matrix'.\n",
    "\n",
    "Stosując One-Hot Encoder używamy tylko kolumn zawierających dane kategoryczne, które chcemy poddać \"kodowaniu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## Aplikujemy OneHotEncoder dla danych kategorycznych:\n",
    "#OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "#OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "#OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "## OneHotEncoder usuwa index; przywracamy go z powrotem:\n",
    "#OH_cols_train.index = X_train.index\n",
    "#OH_cols_valid.index = X_valid.index\n",
    "\n",
    "## Usuwamy stare zmienne kategoryczne ze zbioru treningowego i walidacyjnego a w ich miejsce wstawiamy zmienne \n",
    "## kategoryczne powstałe w OneHotEncoder (patrz: PRZYKŁAD poniżej)\n",
    "#num_X_train = X_train.drop(object_cols, axis=1)\n",
    "#num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "## Określamy wartość MAE:\n",
    "#print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "#print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WNIOSEK:\n",
    "\n",
    "Ostatecznie wybieramy podejście, dla którego wartość MAE jest najmniejsza. Zazwyczaj najlepsze wyniki (najmniejsze MAE) będą dla One-Hot Encoder (Podejście 3), a najgorsze - dla usunięcia kolumn (Podejście 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRZYKŁAD\n",
    "#### Zastosowanie powyższych podejść na zbiorze 'Iowa':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytanie danych\n",
    "X = pd.read_csv('./categorical_variables_dataset/train.csv', index_col='Id')\n",
    "X_test = pd.read_csv('./categorical_variables_dataset/test.csv', index_col='Id')\n",
    "\n",
    "# określenie cech i zmiennej przewidywanej oraz usunięcie wierszy, w których nie określono wartości przewidywanej (do \n",
    "# treningu takie dane są bezużyteczne)\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Dla uproszczenia usuwamy kolumny zawierające braki danych:\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
    "X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_test.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# podział domyślnego zbioru treningowego na zbiór treningowy właściwy i zbiór walidacyjny\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Definicja funkcji, która pozwoli porównać różne podejścia dla danych kategorycznych\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Rozpatrujemy podejścia:\n",
    "\n",
    "##### - Podejście 1 - usunięcie danych kategorycznych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "17837.82570776256\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "# wartość MAE dla zredukowanych danych:\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Podejście 2 - Label Encoding:\n",
    "\n",
    "Przed zastosowaniem Label Encoding sprawdzimy dane, a dokładniej przyjrzymy się dokładniej kolumnie 'Condition2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n",
      "\n",
      "Unique values in 'Condition2' column in validation data: ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n"
     ]
    }
   ],
   "source": [
    "# Wyświetlamy unikalne wartości występujące w kolumnie 'Condition2' dla zb. treningowego i walidacyjnego\n",
    "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
    "print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komentarz:\n",
    "\n",
    "Jak widać, unikalne wartości kolumny 'Condition2' dla zbioru treningowego i walidacyjnego różnią się, dlatego chcąc zastosować Podejście 2 musimy pozbyć się problematycznej kolumny 'Condition2'. Zgodnie z poniższym kodem, kolumny \"problematyczne\" przypisemy do zmiennej 'bad_label_cols', zaś kolumny dla których bez problemów możemy zastosować podejście Label Encoding zostaną zapisane w zmiennej 'good_label_encoding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be label encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'BldgType', 'HouseStyle', 'ExterQual', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Utilities', 'Heating', 'Exterior1st', 'LandSlope', 'Functional', 'HeatingQC', 'SaleType', 'Neighborhood', 'RoofStyle', 'ExterCond', 'Condition2', 'Foundation', 'Exterior2nd', 'RoofMatl', 'Condition1']\n"
     ]
    }
   ],
   "source": [
    "# All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej usuwamy problematyczne kolumny, które nie nadają się dla Podejścia 2, a następnie stosujemy Label Encoding (Podejście 1) dla zbioru treningowego i walidacyjnego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Label Encoding):\n",
      "17575.291883561644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Apply Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in set(good_label_cols):\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "# wartość MAE dla zmodyfikowanych danych:   \n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Podejście 3 - One-Hot Encoding:\n",
    "Zanim zastosujemy podejście O-HE należy sprawdzić liczebność poszczególnych kolumn z danymi kategorycznymi, ponieważ, jak wiadomo, podejście to nie działa dobrze, jeśli zmienna kategoryczna przyjmuje zbyt wiele unikalnych wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Street', 2),\n",
       " ('Utilities', 2),\n",
       " ('CentralAir', 2),\n",
       " ('LandSlope', 3),\n",
       " ('PavedDrive', 3),\n",
       " ('LotShape', 4),\n",
       " ('LandContour', 4),\n",
       " ('ExterQual', 4),\n",
       " ('KitchenQual', 4),\n",
       " ('MSZoning', 5),\n",
       " ('LotConfig', 5),\n",
       " ('BldgType', 5),\n",
       " ('ExterCond', 5),\n",
       " ('HeatingQC', 5),\n",
       " ('Condition2', 6),\n",
       " ('RoofStyle', 6),\n",
       " ('Foundation', 6),\n",
       " ('Heating', 6),\n",
       " ('Functional', 6),\n",
       " ('SaleCondition', 6),\n",
       " ('RoofMatl', 7),\n",
       " ('HouseStyle', 8),\n",
       " ('Condition1', 9),\n",
       " ('SaleType', 9),\n",
       " ('Exterior1st', 15),\n",
       " ('Exterior2nd', 16),\n",
       " ('Neighborhood', 25)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej jasno widać, ile unikalnych wartości zawierają poszczególne kolumny.\n",
    "\n",
    "W przypadku wielkich zbiorów danych (wiele wierszy), podejście O-HE może dodatkowo znacząco go powiększyć (np. dla zbioru o liczbie wierszy 10 000 i kolumnie z danymi kategorycznymi zaw. 100 unikalnych wartości stosujęc O-HE powiększymy pierwotny zbiór o 10 000 * 100 - 10 000 wierszy!!!!!!). Z tego powodu powinno ono być używane tylko dla kolumn z relatywnie małą liczbą unikalnych wartości (< 15). Pozostałe kolumny (> 15) albo usuwamy całkowicie (podejście 1), albo stosujemy Label Encoding (podejście 2).\n",
    "\n",
    "\n",
    "Dla przykładu zastosujemy podejście O-HE jedynie dla kolumn o liczbie unikalnych wartości < 10 ('low_cardinality_cols'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be one-hot encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Neighborhood', 'Exterior1st', 'Exterior2nd']\n"
     ]
    }
   ],
   "source": [
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modyfikujemy zbiór treningowy i walidacyjny zgodnie z powyższym (tylko kolumny 'low_cardinality_cols') oraz obliczamy wartość MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "17525.345719178084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# wartość MAE dla zmodyfikowanych danych:\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) PIPELINES\n",
    "\n",
    "Potoki (ang. pipelines) służą do organizowania (w pewnym sensie porządkowania) naszego kodu, który służy nam do \"preprocessingu\" danych oraz definiowania modeli (Inaczej, potoki obejmują etapy wstępnego przetwarzania danych i modelowania, dzięki czemu jest to w pewnym sensie jeden krok). Oczywiście możliwe jest modelowanie bez używania potoków (jak wyżej), ale posiadają one jednak sporo zalet, np. dzięki ich stosowaniu mamy:\n",
    "\n",
    "- czyściejszy kod\n",
    "- mniej bugów\n",
    "- łatwiejsze wprowadzenie kodu na produkcję\n",
    "- więcej sposobów na walidację modelu (np. walidacja krzyżowa)\n",
    "\n",
    "Przykładowo, przy użyciu potoków łatwo poradzić sobie zarówno z brakami danych, jak i z modyfikacją zmiennych kategorycznych. Potok składa się z 3 kroków:\n",
    "\n",
    "#### Krok 1: Definiujemy etapy obróbki danych (preprocessing steps)\n",
    "\n",
    "Stosując klasę 'ColumnTransformer' możemy \"załatwić\" różne etapy \"preprocessingu\" danych. Kod poniżej:\n",
    "\n",
    "- \"imputuje\" (zastępuje) braki danych w zmiennych numerycznych\n",
    "- \"imputuje\" (zastępuje) braki danych oraz aplikuje podejście O-HE dla zmiennych kategorycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## Preprocessing for numerical data:\n",
    "#numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "## Preprocessing for categorical data:\n",
    "#categorical_transformer = Pipeline(steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "#])\n",
    "\n",
    "## Bundle preprocessing for numerical and categorical data (wszystko razem):\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', numerical_transformer, numerical_cols),\n",
    "#        ('cat', categorical_transformer, categorical_cols)\n",
    "#    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Krok 2: Definiujemy model\n",
    "Poniżej, przykładowo, model Lasów Losowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Krok 3: Tworzymy potok i poddajemy ocenie wynik (evaluation)\n",
    "Poniżej zastosujemy klasę 'Pipeline' do zdefiniowania naszego potoku, który \"załatwi\" nam preprocessing oraz tworzenie modelu. Istotne są:\n",
    "\n",
    "- stosując potoki, dokonujemy obróbki danych treningowych oraz uczymy model w pojedynczej linii kodu (dla porównania, bez potoków chcąc poradzić sobie z brakami danych, zmodyfikować dane kategoryczne (np. O-HE) oraz wytrenować model musimy każdy z tych etapów przeprowadzić osobno)\n",
    "- stosując potoki, przekazujemy nieobrobione cechy (dane) ze zbioru walidacyjnego (X_valid) bezpośrednio do metody 'predict()' i to właśnie potok automatycznie je obrabia zanim dokona prognoz (dla porównania, bez potoków musimy pamiętać, aby przed predykcją poddać obróbce również zbiór walidacyjny!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## Bundle preprocessing and modeling code in a pipeline\n",
    "#my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                              ('model', model)\n",
    "#                             ])\n",
    "\n",
    "## Preprocessing of training data, fit (train) model \n",
    "#my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "## Preprocessing of validation data, get predictions\n",
    "#preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "## Evaluate the model (get MAE)\n",
    "#score = mean_absolute_error(y_valid, preds)\n",
    "#print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WNIOSKI:\n",
    "\n",
    "Potoki są wartościowe z punktu widzenia czystego kodu oraz unikania błędów. Przede wszystkim jednak warto je stosować przy skomplikowanej obróbce danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) CROSS VALIDATION\n",
    "Dotychczasowe podejście, czyli budowa modeli w w oparciu dane (ang. data driven) opierało się na dokonywaniu wyborów dotyczących tego, jakich zmiennych predykcyjnych użyć, jakich typów modeli użyć, jakich cech dostarczyć do tych modeli itp. Takie podejście ma jednak pewne wady. \n",
    "\n",
    "Jedną z nich jest choćby wyodrębnianie zbioru walidacyjnego ze zbioru treningowego. Ogólnie rzecz biorąc, im większy zbiór walidacyjny, tym jest mniej przypadkowości („szumu”) w naszej mierze jakości modelu i tym bardziej będzie ona wiarygodna. Niestety, duży zbiór walidacyjny możemy uzyskać tylko poprzez zabranie wierszy z naszego zbioru treningowego, a im mniejszy zbiór treningowy, tym gorszy model (błędne koło...).\n",
    "\n",
    "W celu uniknięcia \"zużywania\" zbyt dużej ilości danych treningowych na zbiór walidacyjny możemy skorzystać z techniki zwanej **walidacją krzyżową (ang. cross validation)**, która polega na tym, że zbiór treningowy zostaje rozdzielony na uzupełniające się wzajemnie mniejsze podzbiory. Każdy model jest uczony za pomocą różnych kombinacji tych podzbiorów i oceniany przy użyciu pozostałych, nieużytych podzbiorów, co prowadzi do uzyskania wielu miar jakości modelu.\n",
    "\n",
    "Przykładowo, możemy podzielić dane treningowe na 5 podzbiorów (ang. folds), czyli każdy to 20% całego zbioru treningowego:\n",
    "\n",
    "<img src=\"Images/img_6.jpg\">\n",
    "\n",
    "Następnie, uruchamiamy jeden eksperyment dla każdego z nich:\n",
    "* w eksperymencie 1 używamy pierwszego podzbioru, jako zbioru walidacyjnego, a wszystkich pozostałych używamy do trenowania modelu. To daje nam pomiar jakości modelu oparty na 20% początkowych danych treningowych.\n",
    "* w eksperymencie 2 zbiorem walidacyjnym jest drugi podzbiór i z tego eksperymentu uzyskujemy drugą wartość określającą jakość naszego modelu\n",
    "* powtarzamy ten proces do momentu, aż każdy z podzbiorów będzie użyty jako zbiór walidacyjny. Finalnie, 100% danych zostanie użyte do oceny jakości (jako zbiór walidacyjny) co oznacza, że otrzymamy miarę jakość opartą na wszystkich wierszach zestawu danych (nawet, jeśli nie używamy wszystkich wierszy jednocześnie!).\n",
    "\n",
    "#### Kiedy używać walidacji krzyżowej?\n",
    "Walidacja krzyżowa daje dokładniejszą miarę jakości modelu, co jest szczególnie ważne jeśli dokonujemy wiele decyzji jeszcze na etapie modelowania (?). Jest to jednak proces, który może trwać dość długo, ponieważ dokonuje on oceny wielu modeli (w powyższym przykładzie jeden model musi \"przejść\" przez dane 5 razy).\n",
    "\n",
    "Kiedy zatem stosować walidację krzyżową?\n",
    "* w przypadku małych zestawów danych, dla których przeprowadzenie dodatkowych obliczeń (jak wyżej) nie będzie wielkim problemem, ale już podejście \"data driven\" i wyodrębnianie zbioru walidacyjnego ze zbioru treningowego może znacząco wpłynąć na jakość modelu\n",
    "* w przypadku większych zbiorów wystarczy natomiast jeden zestaw walidacyjny - kod będzie działał szybciej a danych będzie wystarczająco dużo i do treningu i do walidacji.\n",
    "\n",
    "Nie istnieje prosty sposób na ocenę, co stanowi duży a co mały zbiór danych. Można przyjąć, że jeśli uruchomienie modelu zajmuje kilka minut lub mniej, to warto skorzystać z walidacji krzyżowej.\n",
    "\n",
    "Można również przeprowadzić walidację krzyżową i sprawdzić czy wyniki dla każdego eksperymentu są do siebie zbliżone - jeśli wyniki różnią się nieznacznie lub są nawet identyczne, prawdopodobnie wystarczy jeden zestaw walidacyjny (podejście \"data driven\").\n",
    "\n",
    "#### Przykład:\n",
    "Stosowanie walidacji krzyżowej bez użycia potoków jest dość problematyczne, dlatego w poniższym przykładzie zastosowano właśnie potoki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USTAWIENIA\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_csv('./categorical_variables_dataset/train.csv', index_col='Id')\n",
    "test_data = pd.read_csv('./categorical_variables_dataset/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = train_data.SalePrice              \n",
    "train_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "X = train_data[numeric_cols].copy()\n",
    "X_test = test_data[numeric_cols].copy()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteka Scikit-Learn zawiera walidację krzyżową pod nazwą 'cross_val_score', gdzie liczbę podzbiorów określa parametr 'cv'. Z kolei parametr 'scoring' określa miarę jakości modelu: w tym przypadku wybraliśmy **ujemny** błąd bezwzględny (negative MAE - dlaczego 'negative'? Konwencja Scikit-Learn).\n",
    "\n",
    "Ponadto, jako że zazwyczaj miarą jakości jest pojedyncza liczba musimy wyciągnąć średnią (mean) spośród wszystkich wartości MAE (scores), jakie otrzymamy dla każdego eksperymentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Całość zawarliśmy w pojedynczej funkcji, która zwraca średnią wartość MAE w zależności od ilości (n_estimators) drzew \n",
    "# w modelu Lasów Losowych:\n",
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    my_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', SimpleImputer()),\n",
    "        ('model', RandomForestRegressor(n_estimators, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    # Multiply by -1 since sklearn calculates *negative* MAE\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                                  cv=3,\n",
    "                                  scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Pętla dla różnych wartości 'n_estimators'\n",
    "results = {}\n",
    "for i in range(1,9):\n",
    "    results[50*i] = get_score(50*i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oceny najlepszej wartości parametru 'n_estimators' dokonano na podstawie wykresu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Najlepsza wartość 'n_estimators' to taka wartość, dla której MAE jest najmniejsze!!! Zatem: \n",
    "n_estimators_best = min(results, key=results.get)\n",
    "print(n_estimators_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PODSUMOWANIE:\n",
    "\n",
    "Stosowanie walidacji krzyżowej daje dużo lepsze wyniki jeśli chodzi o pomiar jakości modelu. Warto zauważyć, że nie trzeba również (stosując walidację i potoki!) zwracać szczególnej uwagi osobno na zbiór treningowy i walidacyjny - zwłaszcza przy małych zbiorach jest to bardzo pomocne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metoda gradientowa\n",
    "\n",
    "Metoda gradientowa (ang. gradient boosting) służy do budowy i optymalizacji modeli. Metoda ta dominuje w konkursach na kaggle.com i zapewnia najlepsze wyniki dla różnych zbiorów danych.\n",
    "\n",
    "Gradient boosting to kolejny, po lasach losowych, przykład \"metody zespołowej\" (ang. ensemble method). Jej działanie opiera się na cyklach - kolejnych iteracjach, które dodają modele do \"zespołu\".\n",
    "\n",
    "Na początku \"zespół\" zawiera jeden model, ktorego wyniki (prognozy) mogą być dość naiwne, jednakże kolejne iteracje uwzględniają popełnione błędy i dodają kolejne modele z \"poprawkami\".\n",
    "\n",
    "Na jeden cykl metody składają się następujące kroki:\n",
    "* w pierwszym kroku używamy bieżącego zespołu (może to być np. pojedynczy model) do generowania prognoz dla każdej obserwacji w zbiorze danych (jeśli modeli w zespole jest > 1, to dodajemy prognozy ze wszystkich modeli w tym zespole)\n",
    "* następnie używamy \"funkcji strat\" (ang. loss function; np. takiej jak Minimum Square Error (MSE)), aby wprowadzić \"poprawki\" do modelu i następnie dodać go (ten poprawiony model) do \"zespołu\" - przede wszystkim określamy parametry modelu tak, by dodanie tego nowego (poprawionego) modelu do zespołu zmniejszyło straty, czyli zwiększyło jego jakość! (Komentarz: \"gradient\" w nazwie metody \"gradient boosting\" oznacza, że do określenia parametrów w tym nowym modelu użyjemy \"metody gradientowej\" (ang. gradient descent) w funkcji straty).\n",
    "* na koniec dodajemy \"poprawiony\" model do zespołu (ang. ensemble) i...\n",
    "* ... powtarzamy cały proces od nowa!\n",
    "\n",
    "<img src=\"Images/img_7.jpg\">\n",
    "\n",
    "#### Biblioteka XGBoost\n",
    "\n",
    "Biblioteka Scikit-Learn posiada implementację metody gradientowej, ale jest ona dosyć uboga, dlatego podczas stosowania metody gradientowej będziemy korzystać z biblioteki **XGBoost** (extreme gradient boosting). Jest to wiodąca biblioteka, jeśli chodzi o pracę z danymi tabelarycznymi (takimi, które można przechowywać w DataFrame w przeciwieństwie do danych takich jak obrazy czy video), która poprzez \"tuning\" parametrów pozwala wytrenować modele o wysokiej jakości.\n",
    "\n",
    "**Ustawienia parametrów:**\n",
    "\n",
    "Biblioteka XGBoost posiada kilka parametrów, które znacznie wpływają na jakość oraz szybkość trenowania danych. Są to m.in.:\n",
    "- **n_estimators** - określa, ile razy należy przejść przez cykl modelowania opisany powyżej. Jest równy liczbie modeli, które włączamy do zespołu. Za niska wartość spowoduje 'underfitting', a kolei za wysoka - 'overfitting'. Typowe wartości mieszczą się w przedziale 100-1000, chociaż to zależy od parametru 'learning_rate' opisanego poniżej\n",
    "- **early_stopping_rounds** - pozwala automatycznie określić optymalną wartość parametru 'n_estimators'. Powoduje zatrzymanie kolejnych iteracji, gdy jakość dla zbioru walidacyjnego przestaje rosnąć (nawet jeśli nie sprawdziliśmy wszystkich wartości 'n_estimators'). Najlepiej jest ustawić wysoką wartość 'n_estimators' oraz użyć parametru 'early_stopping_rounds' w celu znalezienia optymalnego momentu to zatrzymania iteracji. Najrozsądniejszym wyborem jest ustawić 'early_stopping_rounds' = 5 - w takim przypadku zostanie wykonanych jedynie 5 iteracji, które nie polepszają jakości modelu.\n",
    "- **eval_set** - korzystając z parametru 'early_stopping_rounds' konieczne jest również odłożenie \"na bok\" danych niezbędnych do obliczenia jakości na zbiorze walidacyjnym, do tego celu służy właśnie parametr 'eval_set'.\n",
    "- **learning_rate** - zamiast uzyskiwać prognozy po prostu sumując prognozy z każdego modelu składowego, możemy pomnożyć prognozy z każdego modelu przez niewielką liczbę znaną jako współczynnik uczenia (learning_rate) przed ich dodaniem. Oznacza to, że każde kolejne drzewo decyzyjne, które dodajemy do zespołu pomaga nam mniej. Możemy więc ustawić wyższą wartość parametru 'n_estimators' bez ryzyka przetrenowania. Ogólnie, mały współczynnik 'learning_rate' i duża wartość 'n_estimators' dadzą dokładniejsze modele XGBoost, chociaż trenowanie modelu zajmie więcej czasu (dzieje się tak ponieważ wykonuje on więcej iteracji). Domyślna wartość 'learning_rate' = 0.1.\n",
    "- **n_jobs** - mając do czynienia z dużymi zbiorami danych, w przypadku których konieczne jest spędzenie dużej ilości czasu na trenowanie modeli, można użyć **\"przetwarzania równoległego\"**, aby proces budowy modeli był szybszy. Służy do tego parametr 'n_jobs', którego wartość zazwyczaj ustawia się równą liczbie rdzeni komputera. W przypadku małych zbiorów danych jest on jednak bezużyteczny.\n",
    "\n",
    "#### Przykład:\n",
    "\n",
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USTAWIENIA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X = pd.read_csv('./categorical_variables_dataset/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('./categorical_variables_dataset/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice              \n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary (opcjonalne))\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "# One-hot encode the data (to shorten the code, we use pandas)\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_valid = pd.get_dummies(X_valid)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Budowa modelu za pomocą metody gradientowej z domyślnymi parametrami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 17662.736729452055\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Define the model (pozostawiamy póki co domyślne wartości parametrów)\n",
    "my_model_1 = XGBRegressor(random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "my_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions_1 = my_model_1.predict(X_valid)\n",
    "\n",
    "# Calculate MAE\n",
    "mae_1 = mean_absolute_error(predictions_1, y_valid)\n",
    "print(\"Mean Absolute Error:\" , mae_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ulepszenie modelu poprzez określenie parametrów metody gradientowej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 16688.691513270547\n"
     ]
    }
   ],
   "source": [
    "# Define the model (zmieniamy domyślne wartości parametrów 'n_estimators' i 'learning_rate')\n",
    "my_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "\n",
    "# Fit the model\n",
    "my_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions_2 = my_model_2.predict(X_valid)\n",
    "\n",
    "# Calculate MAE\n",
    "mae_2 = mean_absolute_error(predictions_2, y_valid)\n",
    "print(\"Mean Absolute Error:\", mae_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pogorszenie modelu poprzez bezmyślne ustawienie parametrów metody gradientowej\n",
    "\n",
    "Ten krok jest raczej ciekawostką i ma na celu pokazanie, jak nie \"ustawiać\" parametrów, gdyż zazwyczaj prowadzi to do poroszenia modelu (wzrost MAE w porównaniu do \"domyślnego\" kroku 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 127895.0828807256\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "my_model_3 = XGBRegressor(n_estimators=1)\n",
    "\n",
    "# Fit the model\n",
    "my_model_3.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions_3 = my_model_3.predict(X_valid)\n",
    "\n",
    "# Calculate MAE\n",
    "mae_3 = mean_absolute_error(predictions_3, y_valid)\n",
    "print(\"Mean Absolute Error:\", mae_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) DATA LEAKAGE (super ważne!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data leakage** (wyciek danych) potrafi zrujnować nasz model w subtelny sposób, dlatego bardzo ważne jest radzenie spobie z nim w odpowiedni sposób.\n",
    "\n",
    "**Czym jest data leakage?**\n",
    "\n",
    "Wyciek danych ma miejsce, gdy dane treningowe zawierają informację nt zmiennej przewidywanej (target), ale podobne dane nie są dostępne, gdy dochodzi do prognozowania na nowych danych. Prowadzi to do wysokiej jakości modelu na zbiorze treningowym (a możliwe, że na walidacyjnym również), ale model słabo sprawdza się po wprowadzeniu go na produkcję.\n",
    "\n",
    "Innymi słowy, wyciek danych powoduje, że model wygląda w porządku (ma zadowalającą jakość) dopóki nie wprowadzimy go na produkcję (nie zaczniemy na jego podstawie podejmować decyzji) - wtedy staje się on bardzo niedokładny.\n",
    "\n",
    "Istnieją dwa główne rodzaje data leakage:\n",
    "\n",
    "- target leakage\n",
    "- train-test contamination\n",
    "\n",
    "**Target leakage**\n",
    "\n",
    "Ma miejsce, gdy zmienne przewidujące (predyktory - cechy - kolumny) zawierają dane, które nie będą dostępne w momencie podejmowania decyzji (na produkcji). Ważne jest zatem, aby podczas doboru cech do trenowania modelu myśleć w sposób chronologiczny, tzn. czy dana cecha nie pojawia się po uzyskaniu konkretnej wartości zmiennej przewidywanej (target).\n",
    "\n",
    "Przykład: wyobraźmy sobie, że chcemy przewidzieć, kto zachoruje na zapalenie płuc (pneumonia). Wycinek zbioru danych wygląda następująco:\n",
    "\n",
    "<img src=\"Images/img_8.jpg\">\n",
    "\n",
    "_Ludzie biorą antybiotyk PO zachorowaniu, aby wyzdrowieć. Powyższe surowe dane (ang. raw data; przed jakąkolwiek obróbką) pokazują silną korelację pomiędzy kolumnami 'got_pneumonia' i 'took_antibiotic_medicine', pomimo faktu, że kolumna 'took_antibiotic_medicine' została dodana dopiero po zachorowaniu (nikt raczej prewencyjnie leków nie bierze) więc nie ma wpływu na naszą zmienną przewidywaną - TO JEST WŁAŚNIE **DATA LEAKAGE**!_\n",
    "\n",
    "Nie uwzględniając powyższej uwagi i trenując model na danych zawierających kolumnę 'took_antibiotic_medicine' nasz model znalazłby wzorzec, że każda osoba, która nie brała leków nie zachorowała na zapalenie płuc (co jest prawdą, bo leki beirze się dopiero po zachorowaniu a nie przed) - w związku z tym otrzymamy dobrą jakość przewidywań zarówno dla zbioru treningowego, jak i walidacyjnego (jest on wyodrębniony przecież ze zbioru treningowego więc wzorzec ten również będzie obecny). Niestety model będzie bardzo niedokładny po wprowadzeniu go na produkcję, ponieważ zbiór, dla którego będziemy chcieli przewidzieć czy dana osoba zachoruje, czy nie nie będzie zawierał informacji nt. brania leków na zapalenie płuc (branie leków, to nie potencjalna przyczyna choroby!).\n",
    "\n",
    "Wynika stąd, że chcąc uniknąć \"target leakage\" powinniśmy z naszego zbioru treningowego (i walidacyjnego) usunąć cechy, których wartości zostały wprowadzone PO uzyskaniu wartości dla naszej zmiennej przewidywanej (target).\n",
    "\n",
    "<img src=\"Images/img_9.jpg\">\n",
    "\n",
    "**Train-Test Contamination**\n",
    "\n",
    "Ma miejsce, gdy nieuważnie podchodzimy do zbioru treningowego i walidacyjnego. Chodzi o to, że zbiór walidacyjny ma być miarą tego, jak model radzi sobie z danymi, których wcześniej nie brał pod uwagę (nie widział ich wcześniej, są dla niego kompletnie nowe). Przez naszą nieuwagę możemy doprowadzić do tego, że zbiór walidacyjny będzie niestety zawierał dane, które nasz model już zna (część z nich była zawarta w zbiorze treningowym) - to jest właśnie \"Train-Test Contamination\".\n",
    "\n",
    "Przykład: wyobraźmy sobie, że przeprowadzamy preprocessing danych (np. zastępujemy braki danych) przed wywołaniem \"train_test_split\" (przed podziałem danych treningowych na zbiór treningowy i testowy). Jaki jest efekt? Model może uzyskać dobre wyniki na zbiorze testowym, co daje duże zaufanie do niego, ale będzie działał słabo po wdrożeniu na produkcję, dlatego **najpierw \"train_test_split\" a dopiero później modyfikacja danych (nie mylić w ewentualnym \"train_test_split\" na zbiór treningowy właściwy i walidacyjny)!!!** (oprócz wyboru cech i usunięcia wierszy, gdzie 'target' nieznany - to należy wykonać wcześniej, bo na nic nie wpływa).\n",
    "\n",
    "Używanie potoków nie dopuszcza do powstania problemu z \"Train-Test Contamination\".\n",
    "\n",
    "#### Przykłady:\n",
    "\n",
    "https://www.kaggle.com/alexisbcook/data-leakage\n",
    "\n",
    "https://www.kaggle.com/maciejwilk/exercise-data-leakage/edit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
