{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Explainability\n",
    "\n",
    "Poniższy notebook sporządzono na podstawie kursu dostępnego pod adresem https://www.kaggle.com/learn/machine-learning-explainability.\n",
    "\n",
    "Agenda:\n",
    "1. **Use Cases for Model Insights**\n",
    "2. **Permutation Importance**\n",
    "3. **Partial Plots**\n",
    "4. **SHAP Values**\n",
    "5. Advanced Uses of SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) USE CASES FOR MOEDL INSIGHTS\n",
    "\n",
    "### Jakie są możliwe rodzaje wniosków?\n",
    "\n",
    "Większość ludzi uważa, że uczenie maszynowe to tzw. **black boxes**, czyli że owszem, da się uzyskać dzięk niemu dobre prognozy (przewidywania o dobrej jakości), ale nie da się zrozumieć logiki na podstawie, której zostały one przeprowadzone. Aby zrozumieć sens zawarty w algorytmach uczenia maszynowego należy nauczyć się, jak wyciągać wnioski (ang. insights)z modeli.\n",
    "\n",
    "Poniższy kurs skupia się właśnie na metodach wyciągania wniosków z modeli uczenia maszynowego.\n",
    "\n",
    "* Jakie cechy (kolumny) modeli są najważniejsze?\n",
    "* W jaki sposób każda z cech wpływa na każdą prognozę z osobna?\n",
    "* W jaki sposób każda cecha wpływa na przewidywania modelu w ujęciu ogólnym?\n",
    "\n",
    "### Dlaczego te wnioski są istotne?\n",
    "\n",
    "Poprawne wyciąganie wniosków pozwala m.in. na:\n",
    "\n",
    "- debugging\n",
    "- feature engineering\n",
    "- directing future data collection\n",
    "- informing human decision-making\n",
    "- building trust\n",
    "\n",
    "#### Debugging\n",
    "\n",
    "Dane, na których przyjdzie nam pracować, bardzo często są niewiarygodne, zdezorganizowane i ogólnie \"brudne\" (ang. dirty data). Dodatkowym problemem i potencjalnym źródłem błędów jest napisany przez nas kod, który ma na celu \"preprocessing\" tych danych. Dodajmy jeszcze ryzyko pojawienia się \"data leakage\" i jasne staje się, że występowanie błędów (ang. bugs) jest raczej normą niż wyjątkiem.\n",
    "\n",
    "Wniosek:\n",
    "\n",
    "Poprawne debugowanie błędów jest jedną z najcenniejszych umiejętności w Data Science. **Rozumienie wzorców** znalezionych przez model pomaga w identyfikowaniu sytuacji, kiedy są one sprzeczne z otaczającym nas światem (innymi słowy, kiedy nasz model nie działa poprawnie) i jest to zazwyczaj pierwszy krok jeśli chodzi o \"debugging\".\n",
    "\n",
    "#### Informing Feature Engineering\n",
    "\n",
    "_Feature engineering_ jest zazwyczaj najefektywniejszym sposobem na poprawę jakości tworzonych przez nas modeli ML. Zawiera ono tworzenie nowych cech poprzez transformacje surowych danych lub danych utworzonych przez nas wcześniej.\n",
    "\n",
    "Czasem przejdziemy przez cały ten proces opierając się jedynie na naszej intuicji. Niestety jednak w sytuacji, gdy zbiór danych na którym pracujemy jest bardzo rozbudowany (wiele zmiennych) lub temat, nad którym pracujemy jest dla nas nowy, intuicja nie wystarczy.\n",
    "\n",
    "Cytat:\n",
    "\n",
    "_\"A Kaggle competition to 'predict loan defaults' (https://www.kaggle.com/c/loan-default-prediction) gives an extreme example. This competition had 100s of raw features. For privacy reasons, the features had names like f1, f2, f3 rather than common English names. This simulated a scenario where you have little intuition about the raw data._\n",
    "\n",
    "_One competitor found that the difference between two of the features, specificallyf527 - f528, created a very powerful new feature. Models including that difference as a feature were far better than models without it. But how might you think of creating this variable when you start with hundreds of variables?_\n",
    "\n",
    "_The techniques you'll learn in this micro-course would make it transparent that f527 and f528 are important features, and that their role is tightly entangled. This will direct you to consider transformations of these two variables, and likely find the \"golden feature\" of f527 - f528._\n",
    "\n",
    "_As an increasing number of datasets start with 100s or 1000s of raw features, this approach is becoming increasingly important.\"_\n",
    "\n",
    "#### Directing Future Data Collection\n",
    "\n",
    "Nie mamy wpływu na dane pobierane z internetu. Jednakże, wiele organizaji dzięki Data Science ma szansę dowiedzieć się jaki rodzaj danych jest w nich zawarty. Zbieranie i przechowywanie danych może być kosztowne i niewygodne, dlatego ważne jest działanie w taki sposób, aby zbierać jedynie dane istotne z biznesowego punktu widzenia. Wnioski pochodzące z modeli pozwalają na dobre zrozumienie wartości poszczególnych cech zbioru danych, a to umożliwia następnie odkryć co będzie wartościowe w przyszłości (jakie jeszcze dane mogą pomóc w ulepszeniu modelu).\n",
    "\n",
    "#### Informing Human Decision-Making\n",
    "\n",
    "Niektóre decyzje są wynikiem automatycznej analizy wykonanej przez model. Amazon, przykładowo, nie ma zespołu ludzi, którzy za każdym razem decydują, co wyświetlić użytkownikowi na stronie. Jednakże wiele decyzji podejmowanych jest właśnie przez ludzi, ponieważ dzięki poprawnie wyciągniętym wnioskom są oni w stanie tak zoptymalizować model, aby podejmował trafniejsze dezyje (wnioski mogą być więc cenniejsze niż prognozy).\n",
    "\n",
    "#### Building Trust\n",
    "\n",
    "Wiele osób nie będzie ufało naszym modelom bez wcześniejszego zweryfikowania pewnych podstawowych informacji. Jest to mądry środek ostrożności (ang. smart precaution), biorąc pod uwagę częstotliwość występowania błędów w danych. W praktyce, prezentacja wniosków, które pasują do ogólnego zrozumienia problemu, pomoże zbudować zaufanie nawet wśród osób posiadających jedynie powierzchowną wiedzę z zakresu Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PERMUTATION IMPORTANCE\n",
    "\n",
    "Jednym z najważniejszych pytań dotyczących budowy modeli jest: **Jakie cechy (zmienne, kolumny) mają największy wpływ na przewidywania?**\n",
    "\n",
    "Koncept ten nosi nazwę **feature importance**.\n",
    "\n",
    "Istnieje wiele sposobów oceny **feature importance**. Jednym z nich jest **permutation importance** (dosł. waga permutacji). W porównaniu do innych metod jest ona:\n",
    "\n",
    "- szybka do obliczenia\n",
    "- szeroko stosowana i rozumiana\n",
    "- zgodna z właściwościami, które chcielibyśmy, aby miała miara określająca _feature importance_ (ważność poszczególnych cech).\n",
    "\n",
    "### Jak to działa?\n",
    "\n",
    "_Permutation importance_ używa modeli w całkowicie odmienny sposób niż to miało miejsce do tej pory, dlatego jej zrozumienie może za pierwszym razem okazać się trochę problematyczne.\n",
    "\n",
    "Rozważmy następujący zbiór danych:\n",
    "\n",
    "<img src=\"Images/img_48.jpg\">\n",
    "\n",
    "Chcemy przewidzieć, jaki wzrost będzie miała osoba w sieku 20 lat na podstawie danych dot. osób w wieku lat 10.\n",
    "\n",
    "Nasz zbiór zawiera istotne zmienne ( _height at age 10_ ), zmienne mające mały wpływ na przewidywanie (ang. little predictive power) ( _socks owned_ ), a także pozostałe cechy, których nie będziemy tutaj analizować.\n",
    "\n",
    "\n",
    "**WAŻNE:**\n",
    "\n",
    "**_Permutation importance_ jest obliczana po wytrenowaniu modelu!!!** A więc nie będziemy zmieniać modelu ani też nie zmienimy prognoz otrzymanych dla konkretnych obserwacji (tj. dla danej wartości wzrostu, rozmiaru skarpet etc.).\n",
    "\n",
    "Zamiast tego zadamy następujące pytanie: **Jeśli losowo potasuję pojedynczą kolumnę walidacyjnego zbioru danych, pozostawiając cel (target) i wszystkie pozostałe kolumny w niezmienionej formie, to w jaki sposób wpłynie to na dokładność prognoz (w tych przetasowanych danych)?**\n",
    "\n",
    "<img src=\"Images/img_49.jpg\">\n",
    "\n",
    "Losowa zmiana kolejności wartości pojedynczej kolumny powinna powodować mniej dokładne przewidywania (jakość modelu powinna się pogorszyć), ponieważ uzyskany w ten sposób (na skutek tasowania) zbiór danych nie posiada swojego odpowiednika w świecie rzeczywistym. Dokładność przewidywań modelu ulegnie znacznemu pogorszeniu, jeśli pomieszamy kolumnę, która była bardzo istotna z punktu widzenia modelu (model w znacznym stopniu opierał na niej swoje prognozy). W takim przypadku, zmiana kolejności wartości w kolumnie _height at age 10_ zdecydowanie pogorszy jakość przewidywań. Z kolei, jeśli potasujemy kolumnę _socks owned_ spadek jakości będzie dużo mniejszy.\n",
    "\n",
    "Biorąc powyższe pod uwagę, proces wygląda następująco:\n",
    "1. Trenujemy i optymalizujemy nasz model\n",
    "2. Mieszamy (tasujemy) wartości w pojedynczej kolumnie i dokonujemy prognoz na podstawie otrzymanego w ten sposób zbioru danych. Następnie porównujemy te prognozy (dla zbioru po tasowaniu) z prognozami pierwotnymi (przed tasowaniem) i obliczamy, jak bardzo spadła jakość naszego modelu (obliczamy funkcję straty; ang. loss function) - spadek jakości jest tutaj miarą ważności przetasowanej zmiennej (kolumny).\n",
    "3. Przywracamy dane do ich pierwotnej formy (sprzed tasowania) i powtarzamy proces dla każdej kolumny w zbiorze.\n",
    "\n",
    "### Przykład:\n",
    "\n",
    "W poniższym przykładzie wykorzystamy model, który na podstawie statystyk drużyn piłkarskich przewiduje, czy dana drużyna będzie miała w swoich szeregach \"zawodnika meczu\" (budowa modelu nie jest tutaj uwzględniona, a więc poniższy kod ładuje dane i buduje podstawowy model).\n",
    "\n",
    "<img src=\"Images/img_50.jpg\">\n",
    "\n",
    "Poniżej pokazano, jak obliczyć i zilustrować \"wagę\" poszczególnych cech z użyciem biblioteki **eli5**:\n",
    "\n",
    "<img src=\"Images/img_51.jpg\">\n",
    "<img src=\"Images/img_52.jpg\">\n",
    "\n",
    "### Interpretacja wyników\n",
    "\n",
    "Na górze znajdują się cech mające największy wpływ na uzyskiwane prognozy (najważniejsze), a na dole - najmniejszy.\n",
    "\n",
    "Pierwsza liczba w każdym wierszu opisuje, o ile spadła jakość modelu w wyniku tasowania danej kolumny (w tym przypadku przy użyciu \"dokładności\" (ang. accuracy), jako metryki wydajności).\n",
    "\n",
    "Istnieje pewna tolerancja uzyskiwanych wyników zmiany wydajności dla kolejnych tasowań danej cechy (jakość jest sprawdzana dla różnych tasowań danej kolumny, a nie tylko dla jednego), stąd liczba po znaku \"+\\-\" wskazuje, jak średnio zmieniała się wydajność pomiędzy kolejnymi tasowaniami.\n",
    "\n",
    "Czasem wyniki mogą mieć ujemne wartości. W takich przypadkach prognozy dla potasowanych danych okazywały się dokładniejsze (jakość wzrastała) niż dla danych początkowych (bez tasowania). Dziej się tak, gdy dana cecha nie ma żadnego wpływu na przewidywanie (powinna mieć znaczenie bliskie 0), ale przypadek sprawił, że prognozy uzyskane dla potasowanych danych okazały się dokładniejsze. Jest to częściej spotykane w przypadku małych zbiorów danych, ponieważ jest w nich więcej miejsca na \"szczęście\".\n",
    "\n",
    "W naszym przykładzie najważniejszą cechą okazała się \"liczba zdobytych bramek\" i wydaje się to logiczne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) PARTIAL PLOTS\n",
    "\n",
    "Podczas gdy \"waga\" (ang. importance) pokazuje w jakim stopniu zmienna wpływa na przewidywania, **partial dependence plots** (wykresy zależności częściowych) pokazują **_w jaki sposób (jak)_** zmienna wpływa na przewidywania.\n",
    "\n",
    "Pozwala to znaleźć odpowiedź na takie pytania, jak:\n",
    "\n",
    "- Biorąc pod uwagę wszystkie zmienne, jaki wpływ na cenę domu ma długość (longitude) i szerokość (latitude) geograficzna? Jak wyceniane są domu o podobnych cechach, ale zlokalizowane w różnych częściach np. miasta?\n",
    "- Czy przewidywane różnice zdrowotne pomiędzy dwoma grupami wynikają z różnic w diecie, czy z innego powodu?\n",
    "\n",
    "Posiadając wiedzę odnośnie regresji liniowej i regresji logistycznej, _wykresy zależności częściowych_ można interpretować podobnie, jak współczynniki w tych właśnie modelach. Trzeba mieć jednak na uwadze, że _wykresy zależności częściowych_ dla skomplikowanych modeli mogą uchwycić bardziej złożone wzorce niż współczynniki z prostych modeli opartych na regresji.\n",
    "\n",
    "### Jak to działa\n",
    "\n",
    "Podobnie, jak permutation importance, _partial dependence plots_ są obliczane **po wytrenowaniu modelu**!!! Model jest trenowany na prawdziwych danych, które nie były w żaden sposób sztucznie modyfikowane.\n",
    "\n",
    "W przykładzie dotyczącym drużyn piłkarskich mogą się one od siebie różnić na wiele sposobów. Ile strzałów oddał każdy zespół, ile miał podań, ile zdobył bramek itp. Na pierwszy rzut oka, wydaje się trudno określić wpływ tych cech na prognozy zwracane przez model.\n",
    "\n",
    "Aby zobaczyć, w jaki sposób _partial dependence plots_ określają wpływ każdej cechy na przewidywania, wykorzystamy pojedynczy wiersz ze zbioru danych. Przykładowo, wiersz ten może reprezentować zespół, który miał piłkę przez 50% czasu gry, wykonał 100 podań, oddał 10 strzałów i zdobył 1 bramkę.\n",
    "\n",
    "Wykorzystamy wytrenowany model, aby przewidzieć wynik przewidywania (prawdopodobieństwo, że zawodnik tej drużyny zdobędzie tytuł gracza meczu). Jednakże, będziemy dokonywać wielokrotnej zmiany wartości jednej z cech, aby uzyskać serię prognoz (przykładowo będziemy zmieniać wartość posiadania piłki i sprawdzać, jak wpływa to na przewidywania: w pierwszej próbie damy 40%, w następnej 50%, w kolejnej 60% itd.). Prześledzimy otrzymywane wyniki (oś pionowa wykresu) przechodząc od małych wartości posiadania piłki do dożych (oś pozioma).\n",
    "\n",
    "W powyższym opisie dla ułatwienia skupiliśmy się na pojedynczym wierszu danych. Zależności pomiędzy zmiennymi (kolumnami) mogą sprawić, że _wykres zależności częściowych_ dla pojedynczego wiersza będzie nietypowy. Dlatego, wykonamy eksperyment dla wielu wierszy naraz i wykreślimy średni przewidywany wynik na osi pionowej _wykresu_.\n",
    "\n",
    "### Przykład:\n",
    "\n",
    "(Podobnie, jak w punkcie 1 budowa modelu nie jest tutaj istotna, dlatego poniższy kod ładuje dane i buduje podstawowy model.)\n",
    "\n",
    "<img src=\"Images/img_53.jpg\">\n",
    "\n",
    "W pierwszym przykładzie wykorzystamy model oparty od Drzewo Decyzyjne. W codziennym życiu używa się jednak bardziej skomplikowanych modeli.\n",
    "\n",
    "<img src=\"Images/img_54.jpg\">\n",
    "\n",
    "Jak czytać powyższe drzewo:\n",
    "\n",
    "- węzły zawierają kryterium podziału na samej górze (np. Attempts <= 12.5)\n",
    "- para wartości w \"value\" na samym dole oznacza kolejno ilość przewidywań o wartości _False_ i _True_ w konkretnym węźle drzewa\n",
    "\n",
    "_Partial Dependency Plot_ tworzymy z użyciem biblioteki **PDPBox**.\n",
    "\n",
    "<img src=\"Images/img_55.jpg\">\n",
    "\n",
    "Interpretacja wykresu:\n",
    "\n",
    "- Oś y jest interpretowana, jako **zmiana prognozy (zmiana przewidywania)** w stosunku do wartości, jaką byśmy uzyskali dla wartości bazowej lub lewej skrajnej (???)\n",
    "- obszar zacieniowany na niebiesko wskazuje przedział pewności (level of confidence)\n",
    "\n",
    "Na powyższym wykresie możemy zaobserwować, że zdobycie bramki znacznie zwiększa szanse na zdobycie tytułu \"Gracz Meczu\". Jednakże, każda kolejna bramka ma już niewielki wpływ na wartość prognozy.\n",
    "\n",
    "Kolejny przykład:\n",
    "\n",
    "<img src=\"Images/img_56.jpg\">\n",
    "\n",
    "Powyższy wykres wydaje się zbyt prosty, aby mógł być uznany jako wiarygodna reprezentacja rzeczywistości (wynika to jedna z prostoty naszego modelu). Powinieneś być w stanie zaobserwować z powyższego drzewa decyzyjnego, że jest to dokładna reprezentacja struktury naszego modelu (???).\n",
    "\n",
    "Ten sam wykres dla Lasu Losowego:\n",
    "\n",
    "<img src=\"Images/img_57.jpg\">\n",
    "\n",
    "Analizując powyższy wykres dla Lasu Losowego, prawdopodobieństwo zdobycia tytułu \"Gracz Meczu\" rośnie, jeśli zawodnicy przebiegli łącznie do 100km w czasie meczu. Jednakże, przebiegnięcie ponad 100km zmniejsza prawdopodobieństwo.\n",
    "\n",
    "Generalnie, gładki kształt krzywej na wykresie dla Lasu Losowego wydaje się bardziej prawdopodobny niż funkcja skokowa uzyskana dla DT. Zbiór danych jest jednak na tyle mały, że trzeba być ostrożnym w interpretacji dowolnego modelu!\n",
    "\n",
    "### 2D Partial Dependence Plots\n",
    "\n",
    "Jeśli interesują nas zależności pomiędzy zmiennymi (cechami), to wykresy 2D pozwalają nam na ich zbadanie.\n",
    "\n",
    "W poniższym przykładzie ponownie użyjemy modelu DT. Wykres ponownie będzie mało skomplikowany, ale powinieneś być w stanie dostrzec jego podobieństwo do DT.\n",
    "\n",
    "<img src=\"Images/img_58.jpg\">\n",
    "<img src=\"Images/img_59.jpg\">\n",
    "\n",
    "Wykres ilustruje prognozy dla dowolnej kombinacji zmiennych \"Goals Scored\" i \"Distance Covered\".\n",
    "\n",
    "Przykładowo, najwyższa wartość prognoz ma miejsce, gdy drużyna zdobyła 1 bramkę i przebiegła dystans około 100km (dystans ma znaczenie, gdy doszło do zdobycia bramki przez zespół). Jeśli zespół nie zdobył gola, przebiegnięty dystans nie ma żadnego znaczenia na wartość przewidywania (tak czy siak jest on niski i dystans tego nie zmienia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) SHAP VALUES\n",
    "\n",
    "POwyższe punkty pozwalały wyciągnąć ogólne wnioski z modeli uczenia maszynowego, ale co w sytuacji, jeśli chcemy dowiedzieć się, jak działa model dla pojedyczej prognozy?\n",
    "\n",
    "**SHAP (ang. SHape Additive exPlanations) Values** (kształtowanie dodatkowych wyjaśnień?) rozbija prognozę w celu ukazania wpływu pojedynczej zmiennej. Gdzie można użyć tej metody?\n",
    "* Bank jest prawnie zobowiązany wyjaśnić, dlaczego nie udzielił pożyczki danej osobie (SHAPE pozwala znaleźć wyjaśnienie takiej decyzji, która to została podjęta na podstawie prognozy uzyskanej z modelu ML).\n",
    "* Lekarz chce określić, jakie czynniki wpływają na ryzyko wystąpienia jakiejść choroby u pacjenta w celu dostosowania leczenia zapobiegawczego.\n",
    "\n",
    "W tym punkcie użyjemy SHAP Values do wyjaśnienia pojedynczych prognoz. Z kolei punkt następny (5) dotyczyć będzie wykorzystania metody SHAP Values dla całego modelu.\n",
    "\n",
    "### Jak to działa\n",
    "\n",
    "_SHAP Values_ ukazują wpływ konkretnej wartości danej zmiennej (cechy) na prognozę w porównaniu z prognozą, którą wykonalibyśmy gdyby wartość tej zmiennej była wartością bazową (ang. baseline value).\n",
    "\n",
    "Ponownie spróbujemy przewidzieć czy drużyna będzie miała \"Gracza Meczu\".\n",
    "\n",
    "Możemy zadać następujące pytanie:\n",
    "* Jaki wpływ na wynik przewidywania miał fakt, że drużyna zdobyła 3 bramki?\n",
    "\n",
    "Ale łatwiej udzielić konkretnej, liczbowej odpowiedzi, jeśli sformułujemy pytanie następująco:\n",
    "\n",
    "* Jaki wpływ na wynik przewidywania miał fakt, że drużyna zdobyła 3 bramki zamiast \"podstawowej\" liczby bramek?\n",
    "\n",
    "Komentarz: Oczywiście prognoza dla każdej drużyny zalezy o wielu zmiennych, a nie tylko od \"liczby zdobytych bramek\", dlatego możemy powtórzyć proces dla każej zmiennej (cechy).\n",
    "\n",
    "Metoda _SHAP Values_ robi to w sposób, który gwarantuje wynik w łatwej do interpretacji postaci. Rozkłada ona prognozę za pomocą następującej równości:\n",
    "\n",
    "    ANG: _sum(SHAP Values for all features) = pred_for_team - pred_for_baseline_values\n",
    "    PL: _sum(SHAP Values dla wszystkich cech) = prognoza_dla_drużyny - prognoza_dla_wartości_bazowych\n",
    "\n",
    "Oznacza to, że wartości SHAP dla wszystkich cech są sumowane w celu wyjaśnienia, dlaczego nasz prognoza różniła się od prognozy uzyskanej dla wartości bazowych (???). To pozwala nam zilustrować naszą prognozę na wykresie, jak poniżej:\n",
    "\n",
    "<img src=\"Images/img_60.jpg\">\n",
    "\n",
    "Jak interpretować powyższy wykres?\n",
    "\n",
    "Przewidywana wartość wynosi 0.7 dla zespołu (jeśli drużyna zdobędzie 3 bramki, to szansa na gracza meczu wyniesie 70%), podczas wartość bazowa przewidywania, to 0.4979. Wartości cech, które powodują wzrost wartości przewidywania zaznaczono na różowo, zaś ich rozmiar (długość pasków) oznacza wielkość wpływu, jaki miały te cechy na wartość przewidywania. Cechy zmniejszające wartość przewidywania mają kolor niebieski. Największy wpływ na wzrost przewidywania ma zmienna \"Goal Scored\" = 2. Z kolei na spadek prognozy najbardziej wpływa \"Ball Possession\" = 38%.\n",
    "\n",
    "Upraszczając, jeśli odejmiemy od siebie długość pasków różowych i długość pasków niebieskiech, to otrzymamy dystans pomiędzy wartością bazową a prognozą (outcome)???\n",
    "\n",
    "### Kod do obliczania SHAP Values\n",
    "\n",
    "Wykorzystamy bibliotekę **Shap**.\n",
    "\n",
    "(Model znów podstawowy - jak wyżej)\n",
    "\n",
    "<img src=\"Images/img_61.jpg\">\n",
    "\n",
    "Przeanalizujemy _SHAP Values_ dla pojedycznego wiersza ze zbioru danych (wiersz nr 5). Aby mieć porównanie, najpierw rzućmy okiem jaka będzie wartość prognozy dla surowych danych:\n",
    "\n",
    "<img src=\"Images/img_62.jpg\">\n",
    "\n",
    "Jak widać drużyna (wiersz 5) ma 70% szans na zdobycie \"Gracza Meczu\".\n",
    "\n",
    "Teraz sprawdźmy _SHAP Values_ dla pojedynczej prognozy (dalej wiersz 5):\n",
    "\n",
    "<img src=\"Images/img_63.jpg\">\n",
    "\n",
    "Zmienna _shap_values_ jest listą zawierającą dwie tablice (arrays). Pierwsza tablica to wartości SHAP dla prognozy negatywnej (drużyna nie zdobędzie nagrody), druga tablica to z kolei wartości SHAP dla prognozy pozytywnej (zdobędzie nagrodę). Zazwyczaj rozpatrujemy prognozy pod kątem pozytywnego wyniku, dlatego wykorzystamy wartości SHAP dla wyników pozytywnych ( _shap_values[1]_ ).\n",
    "\n",
    "Przeglądanie \"surowych\" tablic jest uciążliwe, ale pakiet _shap_ umożliwia wizualizację wyników:\n",
    "\n",
    "<img src=\"Images/img_64.jpg\">\n",
    "\n",
    "W powyższym przykładzie co prawda skorzystaliśmy z metody _shap_ dla DT ( _shap.TreeExplainer(my_model)_ ), ale oczywiście jest możliwe jej zastosowanie również do innych algorytmów ML:\n",
    "\n",
    "- shap.DeepEcplainer - działa na modelach Deep Learning.\n",
    "- shap.KernelExplainer - działa ze wszystkimi rodzajami modeli, chociaż jest wolniejsza od pozostałych metod oraz zwraca przybliżone (approximation), a nie dokładne wyniki.\n",
    "\n",
    "Poniżej przykład użycia _KernelExplainer_ i podobne wyniki (podobne, bo przybliżone (approximation), ale ogólnie ich sens jest taki sam):\n",
    "\n",
    "<img src=\"Images/img_65.jpg\">\n",
    "<img src=\"Images/img_66.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) ADVANCED USES OF SHAP VALUES\n",
    "\n",
    "W tym punkcie skupimy się na tym, jak wyniki pochodzące z wielu _SHAP Values_ (aggregating many SHAP Values) mogą okazać się bardziej wartościowe od _permutation importance_ i _partial dependence plots_.\n",
    "\n",
    "### SHAP Values Review\n",
    "\n",
    "_SHAP Values_ pokazują, jak bardzo dana cecha wpłynęła na prognozy (w porównaniu do prognozy uzyskanej dla wartości bazowej danej cechy).\n",
    "\n",
    "Rozważmy bardzo prosty model:\n",
    "\n",
    "    y = 4 * x1 + 2 * x2\n",
    "   \n",
    "Jeśli **x1** przyjmuje wartość 2, zamiast wartości bazowej równej 0, to SHAP Value uzyska wartość y = 8 (4 * 2).\n",
    "\n",
    "W praktyce, w przypadku bardziej skomplikowanych modeli, jest to cięższe do obliczenia, ale metoda wartości _SHAP_ pozwala nam rozłożyć daną prognozę na sumę wartości wpływów poszczególnych cech, dając wykres jak poniżej:\n",
    "\n",
    "<img src=\"Images/img_67.jpg\">\n",
    "\n",
    "Ponadto, biblioteka **shap** daje możliwość wizualizacji poszczególnych grup wartości _SHAP_. Wizualizaje te wykazują pewne podobieństwo do _permutation importance_ oraz _partial dependence plots_.\n",
    "\n",
    "### Summary Plots\n",
    "\n",
    "Metoda _permutation importance_ jest świetna, ponieważ jako wynik daje jasne wartości liczbowe, które reprezentują wpływ (wagę) danej cechy na prognozy modelu - pozwala to na łatwe porównywanie cech pomiędzy sobą.\n",
    "\n",
    "Niestety jednak metoda ta nie pozwala dowiedzieć się w jaki sposób dana cecha wpływa na model. Jeśli cecha ma średnią wartość _permutation importance_ może to oznaczać, że ma ona:\n",
    "\n",
    "- duży wpływ na kilka pojedynczych prognoz, ale ogólnie nie ma wpływu na całość przewidywań lub\n",
    "- średni wpływ na wszystkie prognozy.\n",
    "\n",
    "**_SHAP summary plots_** daje nam dokładny wgląd w znaczenie poszczególnych cech (zmiennych) oraz jakie są tego przyczyny. Poniżej przykładowy przykład dot. wcześniejszych danych \"piłkarskich\":\n",
    "\n",
    "<img src=\"Images/img_68.jpg\">\n",
    "\n",
    "Mamy tutaj wykres punktowy. Każdy punkt na wykresie ma 3 cechy:\n",
    "\n",
    "- położenie pionowe (oś pionowa) mówi nam jakiej cechy dotyczy\n",
    "- kolor wskazuje czy ta cecha miała wysoką czy niską wartość z punktu widzenia wiersza zbioru danych, w którym wystąpiła\n",
    "- położenie poziome pokazuje czy wpływ (wagę) tej wartości na prognozę\n",
    "\n",
    "Na przykład, punkt w lewym górnym roku dotyczy zespołu, który zdobył tylko kilka bramek (mało), co wpłyneło na prognozę negatywnie - spowodowało jej spadek o 0.25.\n",
    "\n",
    "Rzeczy, które powinno dać się łatwo wywnioskować z powyższego wykresu, to:\n",
    "\n",
    "- w swoich prognozach model zignorował całkowicie cechy \"Yellow & Red\" oraz \"Red\"\n",
    "- zazwyczaj zmienna \"Yellow Card\" nie ma wpływu na prognozę, choć istnieje skrajny przypadek, gdzie jej wartość spowodowała spadek prognozy (wpłynęła negatywnie na prawdopodobieństwo zdobycia nagrody \"Gracz Meczu\")\n",
    "- wysokie wartości zmiennej \"Goals Scored\" powodowały wzrost prognoz (wzrost prawdopodobieństwa), a nieskie - spadek.\n",
    "\n",
    "### Kod pozwalający uzyskać: Summary Plots\n",
    "\n",
    "<img src=\"Images/img_69.jpg\">\n",
    "\n",
    "Poniższy kod pozwoli nam uzyskać wartości SHAP dla zbioru walidacyjnego (nie jest on zbyt skomplikowany, dlatego wyjaśnienie krok po kroku znajduje się w komentarzach):\n",
    "\n",
    "<img src=\"Images/img_70.jpg\">\n",
    "<img src=\"Images/img_71.jpg\">\n",
    "\n",
    "Kod co prawda nie jest zbyt skomplikowany, ale warto mieć na uwadze:\n",
    "* Wykres tworzymy dla _shap_values[1]_. Dla problemów klasyfikacyjnych istnieje oddzielna tablica wartości SHAP dla każdej możliwej prognozy. W takim przypadku indeksujemy po liście w taki sposób, aby uzyskać wartości SHAP dla prognozy _\"True\"_.\n",
    "* Obliczanie wartości SHAP zazwyczaj zajmuje sporo czasu (ogólnie jest to wolny proces, chyba, że mamy mały zbiór danych). Wyjątkiem jest model z XGBoost, dla którego SHAP ma pewne optymalizacje, które powodują, że proces ten jest szybszy.\n",
    "\n",
    "Powyższy przykład pokazuje, że metoda ta stanowi doskonały przegląd całego modelu. My jednak możemy chcieć zgłębić jedną, wybraną zmienną (cechę) - w tym miejscu do gry wchodzą **_SHAP Dependence Contribution Plots_**.\n",
    "\n",
    "### SHAP Dependence Contribution Plots\n",
    "\n",
    "Poprzednio, za pomocą _Partial Dependence Plots_, sprawdzaliśmy jak pojedyncza cecha wpływa na prognozy modelu. Metoda ta pozwala na wnikliwą i istotną dla wielu rzeczywistych przypadków analizę, ale niestety nie pokazuje ona wszystkiego. Przykładowo, **jaki jest rozkład wag (wpływów)**? **Czy określona wartość cechy ma stały (constant) wpływ na prognozy czy też zmienia się w zależności od wartości innych cech**? Metoda _SHAP dependence contribution plots_ pozwala uzyskać podobny wgląd w model jak _PDP_ , ale jest bardziej szczegółowa.\n",
    "\n",
    "<img src=\"Images/img_72.jpg\">\n",
    "\n",
    "Na początku skupmy się na kształcie, a później na kolorze. Każdy punkt na wykresie reprezentuje pojedynczą obserwację (wiersz danych). Położenie poziome to rzeczywista wartość ze zbioru danych, a położenie pionowe - wpływ tej wartości na prognozę. Swego rodzaju \"nachylenie dodatnie\", które można zaobserować patrząc na ogół obserwacji (kropek) na wykresie dowodzi, że im dłużej drużyna posiada piłkę, tym prawdopodobieństwo zdobycia przez gracza nagrody rośnie.\n",
    "\n",
    "Duży rozstrzał obserwacji sugeruje, że pozostałe cechy (tutaj: Goals Scored) wchodzą w interację (istnieje zależność) z procentem posiadania piłki. Przykładowo poniżej wyróżniliśmy dwa punkty z podobnymi wartościami posiadania piłki. W jednym przypadku mamy jednak ze wzrostem prawdopodobieństwa dla prognozy, a w drugim jego spadek:\n",
    "\n",
    "<img src=\"Images/img_73.jpg\">\n",
    "\n",
    "Dla porównania, prosta regresja liniowa dałaby wykresy, które są idealnymi liniami, bez tego rozrzutu.\n",
    "\n",
    "Sugeruje to, że zagłębiamy się w zależności (interakcje), które reprezentowane są tutaj przez kolory. I chociaż ogólny trend jest wzrostowy (ogólnie im dłużej przy piłce, tym prawdopodobieństwo wyższe), to możemy wizualnie sprawdzić (na podstawie koloru kropki) czy ulega on zmianie.\n",
    "\n",
    "Rozważmy poniższy przykład:\n",
    "\n",
    "<img src=\"Images/img_74.jpg\">\n",
    "\n",
    "Dwa zaznaczone na wykresie punkty wyróżniają się jako leżące daleko od trendu wzrostowego (w pewnym sensie odstają od pozostałych obserwacji). Oba mają kolor fioletowy, który oznacza, że drużyna zdobyła tylko 1 bramkę. Nasza interpretacja może być zatem następująca: **Ogólnie rzecz biorąc, posiadanie piłki zwiększa szansę zdobycia nagrody \"Gracz Meczu\" przez zawodnika danej drużyny, ale jeśli ta drużyna zdobędzie tylko jedną bramkę, to trend ulegnie odwróceniu i \"osoby\" podejmujące decyzję o przyznaniu nagrody mogą stwierdzić, że drużyna nie przełożyła wysokiego posiadania piłki na zdobyte bramki, dlatego nagroda się nie należy**.\n",
    "\n",
    "Ogólnie, poza kilkoma przypadkami odstającymi (ang. outliers) zależność reprezentowana przez kolor kropki nie ma zbyt wielkiego znaczenia. Jednak czasami może całkowicie zmienić trend.\n",
    "\n",
    "### Kod pozwalający uzyskać: SHAP Dependence Contribution Plots\n",
    "\n",
    "W porównaniu do kodu dla _Summary Plot_ zmianie uległa jedynie ostatnia linia:\n",
    "\n",
    "<img src=\"Images/img_75.jpg\">\n",
    "\n",
    "<img src=\"Images/img_76.jpg\">\n",
    "\n",
    "Jeśli nie podamy argumentu dla _interaction_index_ , \"shap\" zastosuje automatyczną logikę, która wybierze taki argument, który będzie najbardziej \"interesujący\".\n",
    "\n",
    "**PODSUMOWANIE**:\n",
    "\n",
    "ZAWSZE MYŚL KRYTYCZNIE O UZYSKIWANYCH WYNIKACH!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
