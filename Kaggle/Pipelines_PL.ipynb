{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czym są i do czego służą potoki?\n",
    "\n",
    "W większości projektów uczenia maszynowego dane, z którymi przyjdzie nam pracować będą wymagały obróbki (nie będą w formacie idealnym do stworzenia najlepszego modelu ML). Bardzo często konieczne jest wykonanie wielu transformacji naszych danych takich, jak kodowanie zmiennych kategorycznych (encoding categorical variables), skalowanie cech (feature scaling) czy normalizacja. Biblioteka Scikit-learn posiada wbudowane funkcje dla większości powszechnie używanych przekształceń.\n",
    "\n",
    "Jednak w typowym projekcie polegającym na budowie modelu ML trzeba bedzie zastosować transformacje danych co najmniej dwa razy - raz podczas trenowania modeli i ponownie dla nowych danych (testowanie modelu). Oczywiście możliwe jest napisanie funkcji, które wykonają wszystkie przekształcenia, ale wymaga to poświęcenia czasu na ich zdefiniowanie, a następnie za każdym razem trzeba je wywoływać dla naszych danych i OSOBNO wywołać model. Narzędziem zdecydowanie upraszczającym ten proces są właśnie **potoki** (ang. pipelines). \n",
    "\n",
    "**Zalety potoków:**\n",
    "\n",
    "- czyściejszy kod (przepływ danych i wykonywane operacje są znacznie łatwiejsze do zrozumienia)\n",
    "- mniej błędów (bugów) i łatwiejsza ich naprawa\n",
    "- potoki wymuszają określoną kolejność operacji w naszym projekcie\n",
    "- łatwiejsze zastosowanie przekształceń dla nowych danych -> łatwiejsze wprowadzenie kodu na produkcję\n",
    "- więcej sposobów na walidację modelu (np. walidacja krzyżowa z użyciem potoków jest łatwiejsza do przeprowadzenia).\n",
    "\n",
    "**Podsumowanie:**\n",
    "\n",
    "Potoki pozwalają wykonać obróbkę danych w dużo bardziej przejrzysty sposób, bez konieczności pisania wielu linii kodu (jest to bardzo przydatne, gdy najpierw obrabiamy dane treningowe, a następnie tę samą obróbkę musimy zastosować do zbioru testowego). Inaczej, potoki obejmują etapy wstępnego przetwarzania i modelowania danych oraz pozwalają dobrać najlepsze możliwe transformacje naszych danych. Ponadto, możliwe jest zastosowanie ich do walidacji i wyboru najlepszego modelu oraz, w połączeniu z *siatką przeszukiwań* (ang. GridSearch), do dostrojenia hiperparametrów naszego modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapy tworzenia potoków\n",
    "\n",
    "1. Pierwszym etapem jest zdefiniowanie wszystkich rodzajów transformacji. Według konwencji tworzymy transformatory (ang. transformers) w zależności od typu danych, który mają modyfikować (jeden dla zmiennych kategorycznych, jeden dla numerycznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# tworzymy listy kolumn numerycznych i kategorycznych\n",
    "# (na tym etapie zb. treningowy nie powinien już zawierać 'targetu' (y_train), bo jego nie chcemy modyfikować)\n",
    "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# tworzymy transformatory\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kolejnym etapem jest zastosowanie utworzonych transformacji na wskazanych kolumnach DF za pomocą metody *ColumnTransformer*.\n",
    "\n",
    "    *ColumnTransformer* pozwala przypisać konkretne transformacje do konkretnych kolumn oraz utworzyć jedną \"przestrzeń cech\" (ang. feature space) po transformacjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "data_transformer = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_features),\n",
    "    ('cat', categorical_transformer, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    OPCJONALNIE: W zależności od rodzaju modelu, który budujemy, może być również konieczna redukcja wymiarów naszej macierzy danych, np. za pomocą PCA. Dlatego poniższy kod pokazuje, jak zastosować PCA z użyciem potoków w nawiązaniu do powyższych punktów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie potoku, które najpierw transformuje dane (jw.), a następnie stosuje PCA\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('data_transformer', data_transformer),\n",
    "    ('reduce_dim', PCA())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Dodawanie algorytmu do potoku (np. klasyfikatora)\n",
    "\n",
    "    Kolejnym krokiem jest połączenie preprocessora dla danych utworzonego powyżej z algorytmem ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Teraz możemy w prosty sposób uruchomić \"trening\" na naszych surowych danych, a utworzony powyżej \"classifier\"\n",
    "# zadba o ich preprocessing\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Następnie predykcja\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli chcemy wytrenować pojedynczy model, to możemy zakończyć budowę modelu na wynikach predykcji z pkt. 3 (jw.). Możemy również wykorzystać potoki do wyboru najlepszego spośród modeli (jn.)\n",
    "\n",
    "4. Wybór najlepszego modelu z domyślnymi parametrami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższy kod zwróci jakość każdego z powyższych klasyfikatorów w zbliżonej formie do poniższej:\n",
    "\n",
    "<img src=\"Images/img_83.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potoki mogą również posłużyć do wyboru najlepszych hiperparametrów dla naszego modelu (jn.).\n",
    "\n",
    "5. Wybór hiperparametrów\n",
    "\n",
    "    Pierwszym krokiem jest utworzenie siatki hiperparametrów dla naszego modelu (param_grid). Ważną rzeczą, o której należy tutaj pamiętać jest konieczność dodania do domyślnej nazwy danego parametru nazwy naszego utworzonego klasyfikatora, czyli w nawiązaniu do powyższych punktów będzie to przedrostek w postaci *classifier__*.\n",
    "\n",
    "    Następnie tworzymy obiekt GridSearch do przeszukiwania siatki parametrów. Musi on zawierać oryginalny potok ( *rf* ), aby uwzględnić wszystkie określone wcześniej transformacje. Kiedy teraz wywołujemy \"trening\"(fit), transformacje zostają zaaplikowane do naszych danych zanim zostanie wywołana walidacja krzyżowa dla siatki przeszukiwań (ang. before a cross-validated grid-search is performed over the parameter grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# założyliśmy, że najlepszym spośród wszystkich sprawdzonych modeli był ten oparty na RandomForestClassifier (rf)\n",
    "# a więc poniższe parametry odnoszą się właśnie do niego.\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [200, 500],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth' : [4,5,6,7,8],\n",
    "    'classifier__criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(rf, param_grid, n_jobs= 1)\n",
    "                  \n",
    "CV.fit(X_train, y_train)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atrykuły:**\n",
    "\n",
    "https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "\n",
    "https://towardsdatascience.com/are-you-using-pipeline-in-scikit-learn-ac4cd85cb27f\n",
    "\n",
    "https://queirozf.com/entries/scikit-learn-pipeline-examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
