{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBRÓBKA DANYCH DLA MODELI ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Obsługa brakujących danych\n",
    "\n",
    "Większość bibliotek ML (włącznie z scikit-learn) zwraca błąd przy próbie budowy modeli na danych, w których występują braki (NA). Poniżej poruszone zostaną 3 podejścia stosowane do pracy z brakującymi danymi.\n",
    "\n",
    "\n",
    "### Rzucenie okiem na dane\n",
    "\n",
    "Pierwszą czynnością pracy z danymi jest ich wczytanie i wstępna (wizualna) analiza:\n",
    "- dataset.head()\n",
    "- dataset.describe()\n",
    "\n",
    "Warto również zapoznać się z **dokumentacją** zbioru danych, jeśli korzystamy ze zbioru utworzonego przez kogoś innego (np. z kaggle.com).\n",
    "\n",
    "### Określenie liczby brakujących danych (NaN, None)\n",
    "\n",
    "Dobrą praktyką jest sprawdzenie czy nasz zbiór danych posiada jakieś braki:\n",
    "\n",
    "<img src=\"Images/img_22.jpg\">\n",
    "\n",
    "Już po pierwszych 10 kolumnach widać, że mamy sporo brakujących wartości. Aby uchwycić skalę problemu najlepiej jest określić **procent** brakujących wartości w naszym zbiorze danych.\n",
    "\n",
    "<img src=\"Images/img_23.jpg\">\n",
    "\n",
    "Widać wyraźnie, że blisko 1/4 komórek jest pusta (NaN lub None).\n",
    "\n",
    "### Określenie skąd się biorą braki danych\n",
    "\n",
    "_\"Look at your data and try to figure out why it is the way it is and how that will affect your analysis\"_\n",
    "\n",
    "Aby sprawnie radzić sobie z brakami danych potrzebne jest doświadczenie (i intuicja!). Dobrym pytaniem, które warto sobie postawić próbując znaleźć odpowiedź na pytanie skąd biorą się brakujące wartości jest: \n",
    "\n",
    "**Czy konkretny brak danych wynika z faktu, że dana wartość nie została zarejestrowana, czy ponieważ w ogóle nie istnieje?**\n",
    "\n",
    "Jeśli mamy do czynienia z brakiem, który wynika z faktu, że dana wartość nie istnieje (np. wzrost najstarszego dziecka kogoś, kto w ogóle nie ma dzieci), to nie ma sensu szukać sposobu na uzupełnienie takiego braku - najlepiej pozostawić wartość NaN.\n",
    "\n",
    "Jeśli jednak brak wynika z tego, że dana wartość nie została zarejestrowana (ale prawdopodobnie może istnieć), wtedy można pokusić się o \"przewidzenie\" brakującej wartości w oparciu o istniejące wartości danej kolumny (cechy) - proces ten nosi nazwę **imputacji (ang. imputation)**.\n",
    "\n",
    "Przykładowo, dla pierwszych 10 kolumn jak wyżej, kolumna _time_ określa liczbę sekund jakie pozostały do końca meczu w momencie wykonania akcji. Wynika stąd, że wszystkie braki w kolumnie _time_ wynikają z tego, że po prostu ich nie zarejestrowano! Stąd, sensowne wydaje się podjąć próbę ich zastąpienia zamiast pozostawić jako NA.\n",
    "\n",
    "Z drugiej strony kolumna _PenalizedTeam_ również zawiera wiele braków, ale w tym przypadku biorą się one z faktu, że po prostu żadna z drużym nie została w danej akcji ukarana (nie było przewinienia), dlatego szukanie \"na siłę\" wartości (drużyny) do wstawienia jest bez sensu i należy pozostawić NA lub ewentualnie wstawić coś a'la \"brak\".\n",
    "\n",
    "**WNIOSEK:**\n",
    "\n",
    "Warto skupić się na każdej kolumnie z osobna i dość szczegółowo przeanalizować w jaki sposób najlepiej poradzić sobie z brakującymi wartościami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podejście 1: Usunięcie wierszy lub kolumn z brakującymi danymi\n",
    "\n",
    "Jedną z najszybszych (ale niekoniecznie najlepszych) metod radzenia sobie z brakami danych jest usunięcie wierszy (lub nawet kolumn, jeśli braków jest ponad 50% w danej kolumnie) zawierających braki.\n",
    "\n",
    "<img src=\"Images/img_1.jpg\">\n",
    "\n",
    "Niestety, poprzez usuwanie całych kolumn znacznie zmniejsza się nasz zbiór danych pod kątem wyboru cech dla modelu (mamy mniej kolumn do wyboru przez co model może mieć później słabą jakość).\n",
    "\n",
    "<img src=\"Images/img_24.jpg\">\n",
    "\n",
    "Jak widać, użycie metody \"dropna\" może spowodować usunięcie wszystkich wierszy, jeśli każdy z nich zawierał przynajmniej jedną brakującą wartość. W takiej sytuacji lepszym rozwiązaniem może być usunięcie kolumn, które posiadają braki:\n",
    "\n",
    "<img src=\"Images/img_25.jpg\">\n",
    "\n",
    "<img src=\"Images/img_26.jpg\">\n",
    "\n",
    "Metoda polegająca na usunięciu kolumn również nie jest najlepsza, bo doprowadziła w powyższym przykładzie do ponad 2-krotnego zmniejszenia naszego zbioru danych...\n",
    "\n",
    "**WSKAZÓWKA:**\n",
    "\n",
    "Metoda całkowitego usuwania brakujących wartości nie jest rekomendowana w pracy przy ważnych komercyjnych projektach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podejście 2: Imputacja - przypisanie nowych wartości\n",
    "\n",
    "Lepsze rozwiązanie niż usuwanie całych kolumn. Polega na zastąpieniu brakujących wartości np. średnią wartością dla danej kolumny czy też wartością najczęściej występującą w danej kolumnie.\n",
    "\n",
    "<img src=\"Images/img_2.jpg\">\n",
    "\n",
    "Wstawienie nowej wartości nie zawsze jest pożądane, ale zazwyczaj prowadzi do uzyskania modeli o wyższej jakości niż w przypadku usuwania całych kolumn.\n",
    "\n",
    "(Poniższy przykład celowo dotyczy wycinka całego zbioru danych, aby pokazać o co chodzi.)\n",
    "\n",
    "<img src=\"Images/img_27.jpg\">\n",
    "\n",
    "Używając modułu Pandas mamy możliwość skorzystania z metody _fillna(n)_ , która zastępuje wszystkie braki danych NAN występujące w DataFramie wskazaną wartością _n_ .\n",
    "\n",
    "<img src=\"Images/img_28.jpg\">\n",
    "\n",
    "Innym, nieco sprytniejszym, sposobem jest zastąpienie braku wartością, która występuje bezpośrednio po nim (po braku) w tej samej kolumnie. Takie działanie ma sens w przypadku zbiorów, w których obserwacje mają jakiś logiczny porządek.\n",
    "\n",
    "<img src=\"Images/img_29.jpg\">\n",
    "\n",
    "W przypadku biblioteki Scikit-Learn możliwe jest zastosowanie metody 'SimpleImputer', która zastąpi braki danych (NaN) w danej kolumnie np. wartością średnią."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podejście 3: Imputacja + INFO - przypisanie nowych wartości oraz uwzgl. info o tym w nowej kolumnie\n",
    "\n",
    "W tym podejściu zastępujemy braki danych, podobnie jak to miało miejsce w punkcie 2, ale oprócz tego dodajemy nową kolumnę  z informacją o tym czy dane w danym wierszu zostały zastąpione nową wartością, czy nie.\n",
    "\n",
    "<img src=\"Images/img_3.jpg\">\n",
    "\n",
    "W pewnych przypadkach, model posiadając wiedzę nt tego, które wartości zostały zastąpione może uzyskiwać lepsze wyniki. W innych - nie zmieni to kompletnie niczego. Przykład:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Obsługa danych liczbowych\n",
    "\n",
    "(**Więcej**: książka, rozdz. 4)\n",
    "\n",
    "Dane ilościowe pozwalają na pomiar wielkości dowolnych elementów: klasy, miesięcznej sprzedaży, pensji czy ocen uczniów. Naturalnym sposobem na przedstawienie tego pomiaru jest użycie liczb, np. 29 uczniów lub sprzedaż w wysokości 529 392 zł. Bardzo ważne jest zatem przekształcenie surowych danych liczbowych na cechy wykorzystywane później w algorytmach ML. \n",
    "\n",
    "Większość algorytmów uczenia maszynowego niestety słabo sobie radzi z atrybutami numerycznymi znajdującymi się w różnych zakresach skali (przykład: całkowita liczba pomieszczeń mieści się w zakresie od 6 do 39 320, z kolei wartości mediany dochodów to zakres zaledwie od 0 do 15). Konieczne jest zatem przeskalowanie cech, **polegające na modyfikacji danych w taki sposób, że mieszczą się one w określonym zakresie**, najczęściej od 0 do 1 lub od -1 do 1. \n",
    "\n",
    "Skalowanie jest jednym z najważniejszych przekształceń dokonywanych na danych liczbowych. Stosuje się je zazwyczaj, gdy używamy algorytmów opartych na miarach odległości między punktami danych, np. **SVM (Support Vector Machines)** lub **K-NN (K-Nearest Neighbours)**. W tych algorytmach zmiana o \"1\" w dowolnej funkcji numerycznej ma taką samą wagę.\n",
    "\n",
    "Przykładowo, nasz zbiór danych może zawierać ceny niektórych produktów w jenach i dolarach. Jeden dolar to około 100 jenów, ale jeśli nie przeskalujemy cen w naszym zbiorze, to algorytmy takie jak SVM czy KNN potraktują różnicę w cenie 1 jena tak samo ważną, jak różnicę 1 dolara!!! A co jeśli mamy wzrost i wagę? Tutaj akurat nie da się jasno określić ile kg powinno równać się jednemu centymetrowi, dlatego skalowanie nie ma zastosowania.\n",
    "\n",
    "Skalowanie umożliwa zatem porównywanie różnych zmiennych \"na równych zasadach\".\n",
    "\n",
    "Wyróżnia się następujące metody skalowania:\n",
    "\n",
    "### Skalowanie min - max\n",
    "\n",
    "**Skalowanie min - max polega na modyfikacji danych w taki sposób, że mieszczą się one w określonym zakresie**, \n",
    "\n",
    "Istnieje wiele technik skalowania, a jedną z najprostszych jest tzw. **skalowanie min - max**. W trakcie tej operacji wartości minimalna i maksymalna cechy są używane do przeskalowania wartości w przedziale. Dokonujemy tego, odejmując od danej wartości wartość minimalną i dzieląc otrzymany wynik przez różnicę wartości maksymalnej i minimalnej. \n",
    "\n",
    "<img src=\"Images/img_77.jpg\">\n",
    "\n",
    "W module Scikit-Learn służy do tego funkcja transformująca *MinMaxScaler*. \n",
    "\n",
    "Zawiera ona hiperparametr *feature_range*, pozwalający zmieniać zakres skali, jeśli z jakiegoś powodu nie odpowiada nam domyślny zakres 0 – 1.\n",
    "\n",
    "**Algorytmy ML wymagające skalowania min-max:**\n",
    "\n",
    "- KNN\n",
    "- regresja liniowa\n",
    "- sieci neuronowe\n",
    "\n",
    "**UWAGA!** Skalowanie min - max jest bardzo wrażliwe na wartości odstające (outliers).\n",
    "\n",
    "(Poniżej przykład skalowania min-max, ale nie z modułu Scikit-Learn a z mlxtend.preprocessing, ale zasada działania taka sama).\n",
    "\n",
    "<img src=\"Images/img_31.jpg\">\n",
    "\n",
    "Zauważmy, że _kształt_ danych na powyższym przykładzie nie zmienił się. Zmianie uległ natomiast zakres wartości (oś pozioma) z 0-8 na 0-1.\n",
    "\n",
    "**Więcej + przykład z Scikit-Learn**: książka, str 73.\n",
    "\n",
    "### Skalowanie robust\n",
    "\n",
    "*RobustScaler* działa podobnie do skalera Min-Max, ale zamiast wartości min i max używa rozstępu międzykwartylnego. A zatem używa mniej danych do skalowania, więc jest bardziej odpowiedni dla sytuacji, gdy dane są odstające.\n",
    "\n",
    "\n",
    "### Normalizacja\n",
    "\n",
    "Terminy skalowania min-max i normalizacji często są (błędnie) stosowane zamiennie. Dzieje się tak, ponieważ oba procesy są bardzo podobne. Zarówno jeden, jak i drugi polega na transformacji danych liczbowych (zmiennych numerycznych) w taki sposób, aby dane te po przekształceniu miały pewne, określone (i pomocne z punktu widzenia modelu) właściwości. \n",
    "\n",
    "Jednym z przykładów normalizacji jest takie przekształcenie danych, że ich suma wynosi 1 (utrzymujemy tzw. jednostkę normatywną). Takie przeskalowanie jest często stosowane w przypadku występowania wielu odpowiedników cech, np. podczas klasyfikacji tekstu, gdy każde słowo lub co n-te słowo jest cechą. \n",
    "\n",
    "W bibliotece Scikit-Learn do tego typu przekształceń służy klasa *Normalizer*.\n",
    "\n",
    "**Algorytmy ML wymagające normalizacji z użyciem klasy *Normalizer*:**\n",
    "\n",
    "Takie przetwarzanie może być użyteczne w przypadku rzadkich zestawów danych (wiele zer) z atrybutami o różnej skali w przypadku korzystania z algorytmów ważących wartości wejściowe, tj.\n",
    "\n",
    "- sieci neuronowe \n",
    "- algorytmy wykorzystujące pomiary odległości, np. KNN.\n",
    "\n",
    "**Więcej + przykład**: książka, str 76.\n",
    "\n",
    "Innym przykładem transformacji rozumianym jako \"normalizacja\", jest **zmianie kształtu rozkładu (shape of the distribution) danych**. Jej celem jest zmiana obserwacji w ten sposób, aby dało się je opisać za pomocą **rozkładu normalnego** (mniej więcej). Jedną z metod pozwalających uzyskać rozkład normalny jest metoda **Box'a-Cox'a**:\n",
    "\n",
    "<img src=\"Images/img_30a.jpg\">\n",
    "<img src=\"Images/img_32.jpg\">\n",
    "\n",
    "Zauważmy, że _kształt_ danych na powyższym przykładzie uległ zmianie. Przed normalizacją był niemal w kształcie litery \"L\", zaś po normalizacji przypomina rozkład normalny (krzywa dzwonowa).\n",
    "\n",
    "Normalizacji danych do rozkładu normalnego można dokonać również z wykorzystaniem pakietu *NumPy* i **logarytmu naturalnego** ( *all_data['norm_fare'] = np.log(all_data.Fare* )\n",
    "\n",
    "Ogólnie rzecz biorąc, normalizację do rozkładu normalnego stosuje się, gdy zamierzamy używać modelu uczenia maszynowego lub techniki statystycznej, która zakłada, że dane posiadają rozkład normalny. \n",
    "\n",
    "**Algorytmy ML wymagające normalizacji do rozkładu normalnego:**\n",
    "\n",
    "- regresja liniowa\n",
    "- naiwny klasyfikator Bayesowski\n",
    "- metody zawierające \"Gaussian\" w nazwie (w ciemno)\n",
    "\n",
    "oraz\n",
    "\n",
    "- analiza wariancji (ANOVA)\n",
    "- liniowa analiza dyskryminacyjna (LDA)\n",
    "\n",
    "\n",
    "### Standaryzacja\n",
    "\n",
    "Standaryzację stosuje się często w celu poprawy wyników klasyfikacji. Metoda ta polega na przesunięciu danych o ich wartość średnią w kierunku zera i późniejszym podzieleniu przez wartość odchylenia standardowego. Inaczej, jest to takie przekształcenie cechy, aby jej średnia wynosiła 0, a odchylenie standardowe 1 (zapewnia to mniej więcej równy rozkład cech). Przekształcona cecha pokazuje o ile odchylenie standardowe wartości początkowej różni się od wartości średniej cechy (jest to tzw. **wskaźnik z-score**)\n",
    "\n",
    "Obliczanie metodą standaryzacji polegaja na tym, że od danej wartości odejmujemy średnią, a następnie wynik dzielimy przez wariancję, dzięki czemu wynikowy rozkład ma wariancję jednostkową.\n",
    "\n",
    "Standaryzacja jest bardzo często stosowaną alternatywą dla skalowania min-max i daje ona bardzo dobre wyniki, jesli zamierzamy korzystać z analizy głównych składowych (PCA). Ponadto, standaryzacja zakłada, że obserwacje **mają rozkład normalny (!)** (patrz: normalizacja).\n",
    "\n",
    "W przeciwieństwie do skalowania min-max, standaryzacja nie ogranicza skalowanych wartości do określonego zakresu, co może stanowić problem dla niektórych algorytmów (np. sieci neuronowe często oczekują wartości wejściowych mieszczących się w zakresie 0-1). Z drugiej strony standaryzacja jest mniej wrażliwa na elementy odstające\n",
    "\n",
    "**Algorytmy ML wymagające standaryzacji:**\n",
    "\n",
    "- SVM (Support Vector Machines)\n",
    "- regresja liniowa\n",
    "- regresja logistyczna\n",
    "- Ogólnie: algorytmy mające lepszą wydajność, jeśli ich cechy posiadają rozkład normalny (Gaussa)\n",
    "\n",
    "W Scikit-Learn standaryzacja występuje jako klasa *StandardScaler*.\n",
    "\n",
    "**Więcej + przykład**: książka, str 74.\n",
    "\n",
    "\n",
    "### Dyskretyzacja (binaryzacja)\n",
    "\n",
    "Przedstawione powyżej metody koncentrowały się na skalowaniu danych. W odróżnieniu od nich *dyskretyzacja* polega na zamianie wartości atrybutów na \"grupy\" (w postaci liczbowej) w zależności od wartości ustalonego progu. I tak, chcąc podzielić dane mamy do dyspozycji dwie techniki:\n",
    "\n",
    "1. Podział cechy liczbowej na podstawie pojedynczej wartości progowej (binaryzacja):\n",
    "\n",
    "Wszystkie wartości większe od progu zamieniane są na 1, a pozostałe - na 0. Pozwala to interpretować obserwacje, jako realizacje schematu Bernoulliego, gdzie 1 oznacza sukces, a 0 porażkę.\n",
    "\n",
    "2. Podział cechy liczbowej na podstawie wielu wartości progowych \n",
    "\n",
    "Otrzymamy więcej grup niż dwie: 0, 1, 2, 3, 4...\n",
    "\n",
    "Dyskretyzacja jest przydatna, gdy istnieje przekonanie, że cecha liczbowa powinna zachowywać się bardziej jak cecha kategoryczna. Przykładowo możemy zakładać niewielką różnicę w nawykach wydawania pieniędzy przez osoby w wieku 19 i 20 lat oraz znacznej przez osoby w wieku 20 i 21 lat (w USA alkohol może kupić dopiero osoba, która ukończyła 21 lat) - w takim przypadku użyteczne może być przeprowadzenie podziału obserwacji na przedstawiające osoby, które mogą i nie mogą spożywać alkoholu (binaryzacja). W innych sytuacjach użyteczna może się okazać dyskretyzacja danych na trzy lub więcej kategorii.\n",
    "\n",
    "W Scikit-Learn standaryzacja występuje jako klasa *Binarizer*.\n",
    "\n",
    "**Więcej + przykład**: książka, str 84.\n",
    "\n",
    "\n",
    "\n",
    "## Outliers (elementy odstające)\n",
    "\n",
    "https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "\n",
    "https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame/23200666#23200666\n",
    "\n",
    "\n",
    "#### Wykrywanie elementów odstających:\n",
    "\n",
    "Niestety nie ma jednej najlepszej techniki przeznaczonej do wykrywania elementów odstających.\n",
    "\n",
    "Chcąc wykryć elementy odstające w poszczególnych cechach można skorzystać z **rozstępu ćwiartkowego (IQR)**. Podaje on różnicę między pierwszą a trzecią ćwiartką zbioru danych i można potraktować go jako rozrzut danych, w którym elementy odstające to obserwacje znacznie oddalone od głównego miejsca koncentracji danych. Za element odstający jest zwykle uznawana każda wartość 1.5 IQR razy mniejsza niż pierwsza ćwiartka lub 1.5 IQR razy większa niż trzecia ćwiartka.\n",
    "\n",
    "**Więcej + przykład**: książka, str 80.\n",
    "\n",
    "#### Obsługa elementów odstających:\n",
    "\n",
    "Podobnie, jak w przypadku wykrywania elementów odstających, także w zakresie ich obsługi nie obowiązuje żadna konkretna reguła.\n",
    "\n",
    "**UWAGA!** Jeśli istnieją elementy odstające, wówczas nie powinno stosować się standaryzacji, która jest wrażliwa na el. odstające. Lepiej zastosować np. klasę *RobustScaler*.\n",
    "\n",
    "**Więcej + przykład**: książka, str 82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Obsługa danych kategorycznych\n",
    "\n",
    "**Więcej**: https://kiwidamien.github.io/encoding-categorical-variables.html\n",
    "\n",
    "**Więcej**: książka, rozdz. 5\n",
    "\n",
    "Bardzo często użyteczne jest mierzenie obiektów nie w kategoriach ich ilości, ale pewnej jakości. Związane z tym informacje są z reguły przedstawiane, jako obserwacje w oddzielnych kategoriach, takich jak np. płeć, kolor lub marka samochodu. Zmienne kategoryczne posiadają zatem organiczoną liczbę wartości. Ogólnie, zmienne kategoryczne są jasno określone i jest ich skończona ilość (jasno wskazują daną kategorię)\n",
    "\n",
    "Jednak nie wszystkie dane kategoryczne są takie same. Zbiór kategorii bez powiązanej z nimi kolejności jest określany mianem kategorii **nominalnej**:\n",
    "\n",
    "- niebieski, czerwony, zielony\n",
    "- mężczyzna, kobieta\n",
    "- banan, truskawka, jabłko\n",
    "\n",
    "Natomiast zbiór kategorii zawierających pewną naturalną kolejność jest określany mianem kategorii **porządkowej**, np.:\n",
    "\n",
    "- mało, średnio, dużo\n",
    "- młody, stary\n",
    "- pozytywny, neutralny, negatywny\n",
    "\n",
    "W Pythonie, jeśli zechcemy użyć zmiennych kategorycznych do budowy modelu ML w ich podstawowej formie, to dostaniemy błąd, dlatego należy poddać je \"preprocessingowi\" i **zamienić je na wartości liczbowe**. \n",
    "\n",
    "Poniżej opis wybranych podejść pracy ze zmiennymi kategorycznymi.\n",
    "\n",
    "### Całkowite usunięcie cech kategoryzujących:\n",
    "\n",
    "Najłatwiejszym rozwiązaniem jest usunięcie zmiennych (kolumn) zawierających wartości kategoryczne. Działa tylko, jeśli kolumny nie zawierają informacji istotnych z punktu widzenia modelu.\n",
    "\n",
    "### Kodowanie nominalnych cech kategoryzujących:\n",
    "\n",
    "**One-Hot Encoding / Dummying (kodowanie \"gorącojedynkowe\")**\n",
    "\n",
    "Polega na utworzeniu nowego zestawu kolumn, który jasno określa czy dana wartość zmiennej kategorycznej występuje w zbiorze danych, czy nie (jest to tworzenie cechy binarnej dla każdej klasy oryginajnej cechy). Oznacza to, że każda klasa staje się samodzielną cechą binarną, gdzie 1 oznacza wystąpienie danej cechy, a 0 - brak.\n",
    "\n",
    "<img src=\"Images/img_5.jpg\">\n",
    "\n",
    "One-Hot Encoding nie porządkuje kategorii, dlatego podejście to sprawdza się, gdy nie jest jasny porządek wartości zmiennej kategorycznej (np. \"Red\" nie jest ani większy ani zmiejszy od \"Yellow\")\n",
    "\n",
    "Podejście One-Hot Encoding nie działa dobrze, jeśli zmienna kategoryczna przyjmuje zbyt wiele unikalnych wartości (zakłada się, że nie używa się tego podejścia jeśli unikalnych wartości > 15. \n",
    "\n",
    "W Scikit-Learn: *LabelBinarizer*, *MultilabelBinarizer* lub *OneHotEncoder*.\n",
    "\n",
    "**Więcej**: książka, str 92\n",
    "\n",
    "**Więcej**: notatnik *Intermediate Machine Learning*\n",
    "\n",
    "### Kodowanie porządkowych cech kategoryzujących:\n",
    "\n",
    "**Label Encoding**\n",
    "\n",
    "Polega na przypisaniu etykietom wartości liczbowych (int). Jest to swego rodzaju porządkowanie etykiet, np. \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "\n",
    "<img src=\"Images/img_4.jpg\">\n",
    "\n",
    "Powyższe podejście ma sens, ponieważ kategorie (etykiety) mają swój \"ranking\". Nie wszystkie zmienne kategoryczne mają jednak wartości, które da się uporządkować (inaczej, nie wszystkie zmienne kategoryczne działają, jak zmienne porządkowe!). W przypadku Drzew i Lasów można jednak zakładać, że kodowanie (jw) zmiennych porządkowych przyniesie dobry wynik.\n",
    "\n",
    "WAŻNE!\n",
    "Jeśli w kolumnie zbioru walidacyjnego występują dane kategoryczne, których wartości nie występują w tej samej kolumnie zbioru treningowego, to otrzymamy BŁĄD! Najprostszym rozwiązaniem w takiej sytuacji jest po prostu całkowite usunięcie problematycznej kolumny.\n",
    "\n",
    "**Więcej**: książka, str 94\n",
    "\n",
    "**Więcej**: notatnik *Intermediate Machine Learning*\n",
    "\n",
    "\n",
    "\n",
    "### Inne metody kodowania cech kategoryzujących:\n",
    "\n",
    "**Więcej**: notatnik *Feature_Engineering*\n",
    "\n",
    "#### Count Encoding\n",
    "\n",
    "Polega na zastąpieniu każdej z wartości kategorycznych liczbą jej wystąpień w zbiorze danych. Przykładowo, jeśli wartość \"GB\" pojawia się w zbiorze 10 razy, to każde wystąpienie \"GB\" zostanie zastąpione liczbą 10. \n",
    "\n",
    "Chcąc skorzystać z tej metody należy użyć biblioteki 'category_encoders' oraz metody 'CountEncoder', która działa z metodami '.fit' oraz '.transform' pochodzącymi z pakietu Scikit-Learn.\n",
    "\n",
    "<img src=\"Images/img_10.jpg\">\n",
    "\n",
    "#### Target Encoding\n",
    "\n",
    "Polega na zastąpieniu wartości kategorycznych średnią wartością obliczoną na podstawie poszczególnych wartości zmiennej przewidywanej (target) dla danej wartości zmiennej kategorycznej (inaczej, grupuje unikalne wartości zmiennej kategorycznej, oblicza dla każdej grupy średnią wartość targetu i wstawia w każdy wiersz należący do tej grupy).\n",
    "\n",
    "Ta technika wykorzystuje zmienną docelową (target) do utworzenia nowej cechy (która ma zastąpić cechę \"kategorczną\"), dlatego wykorzystanie tej nowej cechy do treningu lub walidacji modelu byłoby w pewnym sensie **wyciekiem danych** (data leakage). Należy zatem zastosować \"target encodings\" jedynie dla zbioru treningowego.\n",
    "\n",
    "Chcąc skorzystać z tej metody należy użyć biblioteki 'category_encoders' oraz metody 'TargetEncoder', która działa z metodami '.fit' oraz '.transform' pochodzącymi z pakietu Scikit-Learn.\n",
    "\n",
    "<img src=\"Images/img_11.jpg\">\n",
    "\n",
    "#### CatBoost Encoding\n",
    "\n",
    "Metoda ta jest podobna do \"Target Encoding\", ponieważ również opiera się na wartościach zmiennej przewidywanej (target), jednakże stosując \"CatBoost Encoding\" wartość prawdopodobieństwa jest obliczana jedynie na podstawie wierszy poprzedzających wiersz, dla którego chcemy obliczyć nową wartość zmiennej kategorycznej.\n",
    "\n",
    "<img src=\"Images/img_12.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
